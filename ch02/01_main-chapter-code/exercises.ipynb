{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: '../requirements.txt'\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 BPE of unknown words\n",
    "Try the BPE tokenizer from the tiktoken library on the unknown words “Akwirw ier” \n",
    "and print the individual token IDs. \n",
    "\n",
    "Then, call the decode function on each of the resulting integers in this list \n",
    "to reproduce the mapping shown in figure 2.11. \n",
    "\n",
    "Lastly, call the decode method on the token IDs to \n",
    "check whether it can reconstruct the original input, “Akwirw ier.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33901, 86, 343, 86, 220, 959, 13]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s =\"Akwirw ier.\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "tokens = tokenizer.encode(s)\n",
    "tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Akwirw ier.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementing the GPT DataLoader class\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    " \n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            stride: the number of indexes to move between batches\n",
    "            max_length: the window size\n",
    "        \"\"\"\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i: i+max_length]\n",
    "            target_chunk = token_ids[i+1: i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMs-from-scratch  examples  getting_started.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"LLMs-from-scratch/ch02/01_main-chapter-code/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "encoded_text = tokenizer.encode(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5145"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GPTDatasetV1(raw_text, tokenizer, max_length=2, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([33901,    86]), tensor([ 86, 343]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([343,  86]), tensor([ 86, 220]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "context_length = 1024\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length, stride=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.7198e+00,  2.3616e+00, -2.3608e-01,  ..., -2.1460e+00,\n",
      "           5.5452e-01, -1.2040e+00],\n",
      "         [-1.4681e+00, -4.6416e-01, -1.3035e+00,  ...,  7.8037e-01,\n",
      "          -1.0259e+00, -5.8477e-01],\n",
      "         [ 2.9149e-01,  1.2875e-01,  5.5797e-02,  ..., -5.2512e-01,\n",
      "          -1.4244e+00,  2.5446e+00],\n",
      "         [-8.3724e-02,  1.2154e+00, -4.4828e-01,  ..., -9.9454e-01,\n",
      "          -4.0514e-01, -6.3021e-01]],\n",
      "\n",
      "        [[-8.7138e-01,  1.6718e+00,  1.3357e+00,  ...,  6.0381e-01,\n",
      "           6.0504e-01, -1.5115e+00],\n",
      "         [-2.1012e+00,  3.7795e-01, -8.5915e-01,  ...,  7.4455e-01,\n",
      "           2.4568e+00, -1.8294e+00],\n",
      "         [-2.0261e+00,  2.4717e+00, -4.1007e-02,  ...,  1.0370e+00,\n",
      "          -1.3871e+00,  1.0243e+00],\n",
      "         [-8.0761e-04,  2.0683e+00, -1.5610e+00,  ...,  7.7741e-02,\n",
      "           7.4035e-01, -9.1926e-01]],\n",
      "\n",
      "        [[-1.5308e+00,  1.6588e+00,  4.8230e-01,  ...,  1.4478e+00,\n",
      "           6.5506e-01, -2.4078e-01],\n",
      "         [ 3.7883e-01, -2.4422e-01,  5.2323e-01,  ...,  6.9672e-01,\n",
      "           7.3909e-01, -1.7297e+00],\n",
      "         [-2.0468e+00,  5.6440e-01,  9.1291e-01,  ...,  1.6473e+00,\n",
      "          -1.0619e+00,  1.9273e-01],\n",
      "         [ 8.9930e-01,  2.6121e+00, -2.4207e+00,  ...,  1.7760e+00,\n",
      "           2.0051e+00,  5.6657e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-7.1858e-01,  1.6678e+00,  1.1904e+00,  ...,  7.5253e-01,\n",
      "          -1.5012e-01, -8.2586e-01],\n",
      "         [-2.3866e+00, -1.6245e+00, -2.1736e+00,  ...,  2.5243e-01,\n",
      "           1.6572e+00, -1.5697e+00],\n",
      "         [ 1.5249e-01, -9.0055e-01, -4.0924e-01,  ..., -2.8761e-01,\n",
      "           3.1875e-01,  1.8416e+00],\n",
      "         [-1.8158e-01,  1.9979e+00,  2.8653e-02,  ...,  1.0114e+00,\n",
      "           1.4629e+00, -1.8031e+00]],\n",
      "\n",
      "        [[ 9.6906e-02,  3.3506e+00, -2.0981e+00,  ...,  6.2607e-01,\n",
      "           5.5471e-01,  3.0591e-02],\n",
      "         [-3.9371e+00, -9.1082e-02, -2.0430e+00,  ..., -1.0578e+00,\n",
      "           1.0971e+00, -1.6244e+00],\n",
      "         [-2.7028e-01, -6.0439e-01, -4.1120e-01,  ...,  8.9554e-01,\n",
      "           2.6544e-01,  1.7830e+00],\n",
      "         [ 8.9930e-01,  2.6121e+00, -2.4207e+00,  ...,  1.7760e+00,\n",
      "           2.0051e+00,  5.6657e-01]],\n",
      "\n",
      "        [[ 4.5409e-01,  1.6950e+00,  6.9958e-01,  ..., -3.1930e-01,\n",
      "           1.3204e+00, -1.3795e+00],\n",
      "         [-2.2667e+00, -1.5517e+00, -8.1047e-01,  ...,  1.9009e+00,\n",
      "           1.2827e+00, -1.1823e+00],\n",
      "         [-1.2471e+00, -2.9207e-01,  1.1361e+00,  ...,  2.3202e-01,\n",
      "           2.2176e+00,  4.7464e-01],\n",
      "         [ 1.1786e+00, -4.5289e-01, -1.9964e+00,  ..., -1.9026e-01,\n",
      "           1.2638e+00,  9.8479e-02]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    x, y = batch\n",
    "    #print(x, y)\n",
    "    token_embeddings = token_embedding_layer(x)\n",
    "    pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "\n",
    "    input_embeddings = token_embeddings + pos_embeddings\n",
    "    print(input_embeddings)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
