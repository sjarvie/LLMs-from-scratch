{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45398736-7e89-4263-89c8-92153baff553",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd524e-864c-4012-b0a2-ccfc56e80024",
   "metadata": {
    "id": "66dd524e-864c-4012-b0a2-ccfc56e80024"
   },
   "source": [
    "# Chapter 5: Pretraining on Unlabeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92b989e9-da36-4159-b212-799184764dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.8.2\n",
      "numpy version: 1.26.4\n",
      "tiktoken version: 0.7.0\n",
      "torch version: 2.2.1+cu121\n",
      "tensorflow version: 2.16.2\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\", \n",
    "        \"numpy\", \n",
    "        \"tiktoken\", \n",
    "        \"torch\",\n",
    "        \"tensorflow\" # For OpenAI's pretrained weights\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3bdf9e-2ff0-4a57-abab-ede2d955a237",
   "metadata": {},
   "source": [
    "- In this chapter, we implement the training loop and code for basic model evaluation to pretrain an LLM\n",
    "- At the end of this chapter, we also load openly available pretrained weights from OpenAI into our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd27fcc-2886-47cb-b544-046c2c31f02a",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/chapter-overview.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d214765-7a73-42d5-95e9-302154b29db9",
   "metadata": {},
   "source": [
    "- The topics covered in this chapter are shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67711d4-8391-4fee-aeef-07ea53dd5841",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model--0.webp\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d824183-145c-4865-89e1-1f0d0a338f19",
   "metadata": {
    "id": "0d824183-145c-4865-89e1-1f0d0a338f19"
   },
   "source": [
    "## 5.1 Evaluating generative text models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3350f8c-5181-4f9b-a789-4523105e98f2",
   "metadata": {},
   "source": [
    "- We start this section with a brief recap of initializing a GPT model using the code from the previous chapter\n",
    "- Then, we discuss basic evaluation metrics for LLMs\n",
    "- Lastly, in this section, we apply these evaluation metrics to a training and validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc1cf3f-82d8-46c7-9ecc-58979ce87cdd",
   "metadata": {
    "id": "bdc1cf3f-82d8-46c7-9ecc-58979ce87cdd"
   },
   "source": [
    "### 5.1.1 Using GPT to generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3415fd-9f4a-4548-908e-9dfa56edc9bc",
   "metadata": {},
   "source": [
    "- We initialize a GPT model using the code from the previous chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86000d74-624a-48f0-86da-f41926cb9e04",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86000d74-624a-48f0-86da-f41926cb9e04",
    "outputId": "ad482cfd-5a62-4f0d-e1e0-008d6457f512"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c6cf0f-7458-48a2-97fd-aa5068d65e8c",
   "metadata": {},
   "source": [
    "- We use dropout of 0.1 above, but it's relatively common to train LLMs without dropout nowadays\n",
    "- Modern LLMs also don't use bias vectors in the `nn.Linear` layers for the query, key, and value matrices (unlike earlier GPT models), which is achieved by setting `\"qkv_bias\": False`\n",
    "- We reduce the context length (`context_length`) of only 256 tokens to reduce the computational resource requirements for training the model, whereas the original 124 million parameter GPT-2 model used 1024 tokens\n",
    "  - This is so that more readers will be able to follow and execute the code examples on their laptop computer\n",
    "  - However, please feel free to increase the `context_length` to 1024 tokens (this would not require any code changes)\n",
    "  - We will also load a model with a 1024 `context_length` later from pretrained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f80895-be35-4bb5-81cb-f357ef7367fe",
   "metadata": {},
   "source": [
    "- Next, we use the `generate_text_simple` function from the previous chapter to generate text\n",
    "- In addition, we define two convenience functions, `text_to_token_ids` and `token_ids_to_text`, for converting between token and text representations that we use throughout this chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741881f3-cee0-49ad-b11d-b9df3b3ac234",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/gpt-process.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e062b82-3540-48ce-8eb4-009686d0d16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnÙ… refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from previous_chapters import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d3249b-b2a0-44c4-b589-ae4b403b8305",
   "metadata": {},
   "source": [
    "- As we can see above, the model does not produce good text because it has not been trained yet\n",
    "- How do we measure or capture what \"good text\" is, in a numeric form, to track it during training?\n",
    "- The next subsection introduces metrics to calculate a loss metric for the generated outputs that we can use to measure the training progress\n",
    "- The next chapters on finetuning LLMs will also introduce additional ways to measure model quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955f9e1a-7bf7-40d8-b1fa-eacabdee8d8e",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3d7ea2-637f-4490-bc76-e361fc81ae98",
   "metadata": {
    "id": "0f3d7ea2-637f-4490-bc76-e361fc81ae98"
   },
   "source": [
    "### 5.1.2 Calculating the text generation loss: cross-entropy and perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1ba8aa-fb03-4d25-957f-fe8778762440",
   "metadata": {},
   "source": [
    "- Suppose we have an `inputs` tensor containing the token IDs for 2 training examples (rows)\n",
    "- Corresponding to the `inputs`, the `targets` contain the desired token IDs that we want the model to generate\n",
    "- Notice that the `targets` are the `inputs` shifted by 1 position, as explained in chapter 2 when we implemented the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b5402f8-ec0c-4a44-9892-18a97779ee4f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b5402f8-ec0c-4a44-9892-18a97779ee4f",
    "outputId": "8d6fa0ff-7b37-4634-c3f0-2c050cbe81f0"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dc0645-ac2c-4973-9b40-6da40515bede",
   "metadata": {},
   "source": [
    "- Feeding the `inputs` to the model, we obtain the logits vector for the 2 input examples that consist of 3 tokens each\n",
    "- Each of the tokens is a 50,257-dimensional vector corresponding to the size of the vocabulary\n",
    "- Applying the softmax function, we can turn the logits tensor into a tensor of the same dimension containing probability scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7b6ec51-6f8c-49bd-a349-95ba38b46fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c36a382-b5e2-4de6-9e65-0b69b685013b",
   "metadata": {},
   "source": [
    "- The figure below, using a very small vocabulary for illustration purposes, outlines how we convert the probability scores back into text, which we discussed at the end of the previous chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384d86a9-0013-476c-bb6b-274fd5f20b29",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/proba-to-text.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8480efd-d419-4954-9ecc-2876055334bd",
   "metadata": {},
   "source": [
    "- As discussed in the previous chapter, we can apply the `argmax` function to convert the probability scores into predicted token IDs\n",
    "- The softmax function above produced a 50,257-dimensional vector for each token; the `argmax` function returns the position of the highest probability score in this vector, which is the predicted token ID for the given token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b84c9f-dd08-482e-b903-a86fe44e1144",
   "metadata": {},
   "source": [
    "- Since we have 2 input batches with 3 tokens each, we obtain 2 by 3 predicted token IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34ebd76a-16ec-4c17-8958-8a135735cc1c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34ebd76a-16ec-4c17-8958-8a135735cc1c",
    "outputId": "ed17da47-c3e7-4775-fd00-4ec5bcda3db2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee4072c-21ed-4df7-8721-dd2535362573",
   "metadata": {},
   "source": [
    "- If we decode these tokens, we find that these are quite different from the tokens we want the model to predict, namely the target tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c990ead6-53cd-49a7-a6d1-14d8c1518249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53eb8a7-070e-46d6-930c-314ba55a6ff2",
   "metadata": {},
   "source": [
    "- That's because the model wasn't trained yet\n",
    "- To train the model, we need to know how far it is away from the correct predictions (targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad90592f-0d5d-4ec8-9ff5-e7675beab10e",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/proba-index.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7251bf5-a079-4782-901d-68c9225d3157",
   "metadata": {},
   "source": [
    "- The token probabilities corresponding to the target indices are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54aef09c-d6e3-4238-8653-b3a1b0a1077a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54aef09c-d6e3-4238-8653-b3a1b0a1077a",
    "outputId": "41c946a2-c458-433e-a53d-5e7e89d9dddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4540e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e89a19-73c2-4e49-93b4-861f699f1cbf",
   "metadata": {},
   "source": [
    "- We want to maximize all these values, bringing them close to a probability of 1\n",
    "- In mathematical optimization, it is easier to maximize the logarithm of the probability score than the probability score itself; this is out of the scope of this book, but I have recorded a lecture with more details here: [L8.2 Logistic Regression Loss Function](https://www.youtube.com/watch?v=GxJe0DZvydM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31402a67-a16e-4aeb-977e-70abb9c9949b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31402a67-a16e-4aeb-977e-70abb9c9949b",
    "outputId": "1bf18e79-1246-4eab-efd8-12b328c78678"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "# Compute logarithm of all token probabilities\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4261441-a511-4633-9c4c-67998af31b84",
   "metadata": {},
   "source": [
    "- Next, we compute the average log probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b003797-161b-4d98-81dc-e68320e09fec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b003797-161b-4d98-81dc-e68320e09fec",
    "outputId": "a447fe9c-7e27-40ed-f1fb-51210e3f7cc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average probability for each token\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cef46a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36d51994-ad17-4ba3-a6ec-f588b4b13585",
   "metadata": {},
   "source": [
    "- The goal is to make this average log probability as large as possible by optimizing the model weights\n",
    "- Due to the log, the largest possible value is 0, and we are currently far away from 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de388a1-8a0a-4c94-8894-9041dc6ad514",
   "metadata": {},
   "source": [
    "- In deep learning, instead of maximizing the average log-probability, it's a standard convention to minimize the *negative* average log-probability value; in our case, instead of maximizing -10.7722 so that it approaches 0, in deep learning, we would minimize 10.7722 so that it approaches 0\n",
    "- The value negative of -10.7722, i.e., 10.7722, is also called cross-entropy loss in deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "176ddf35-1c5f-4d7c-bf17-70f3e7069bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340a9491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84eeb868-abd8-4028-82db-107546bf7c2c",
   "metadata": {},
   "source": [
    "- PyTorch already implements a `cross_entropy` function that carries out the previous steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd24b7f-b760-47ad-bc84-86d13794aa54",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/cross-entropy.webp?123\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aaf9dd-3ee6-42bf-a63f-6e93dbfb989d",
   "metadata": {},
   "source": [
    "- Before we apply the `cross_entropy` function, let's check the shape of the logits and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "695d6f64-5084-4c23-aea4-105c9e38cfe4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "695d6f64-5084-4c23-aea4-105c9e38cfe4",
    "outputId": "43fd802a-8136-4b35-df0d-f61a5d4cb561"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd678556",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise, compute cross entropy loss myself\n",
    "\n",
    "# 1. compute soft max probabilities\n",
    "probsa = torch.softmax(logits, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "374d7681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. compute log of soft max probability\n",
    "logprobsa = torch.log(probsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af57abe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 50257])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobsa.flatten(0,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e934e23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.flatten().unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7e921b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Use the target sto gather the log probabilities of the correct class\n",
    "# e.g. generate a tensor of probailities for each of the target tokens\n",
    "\n",
    "targetlogprobsa = logprobsa.flatten(0,1).gather(dim=1, index=targets.flatten().unsqueeze(-1)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4099230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Compute negative log likelihood\n",
    "negative_log_likelihood = -targetlogprobsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f47bfb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.7940)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Average the loss\n",
    "loss = negative_log_likelihood.mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54cb9427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3626,  6100,   345],\n",
       "        [ 1107,   588, 11311]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3d65f0-6566-4865-93e4-0c0bcb10cd06",
   "metadata": {},
   "source": [
    "- For the `cross_entropy` function in PyTorch, we want to flatten these tensors by combining them over the batch dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e17e027-ab9f-4fb5-ac9b-a009b831c122",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e17e027-ab9f-4fb5-ac9b-a009b831c122",
    "outputId": "0b2b778b-02fb-43b2-c879-adc59055a7d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4921a57f-3a79-473e-a863-6d63b495010f",
   "metadata": {},
   "source": [
    "- Note that the targets are the token IDs, which also represent the index positions in the logits tensors that we want to maximize\n",
    "- The `cross_entropy` function in PyTorch will automatically take care of applying the softmax and log-probability computation internally over those token indices in the logits that are to be maximized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62d0816e-b29a-4c8f-a9a5-a167562de978",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62d0816e-b29a-4c8f-a9a5-a167562de978",
    "outputId": "c0be634a-2c65-4ff7-a73f-1bfc2e406ba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f15ce17-fd7b-4d8e-99da-b237523a7a80",
   "metadata": {},
   "source": [
    "- A concept related to the cross-entropy loss is the perplexity of an LLM\n",
    "- The perplexity is simply the exponential of the cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "168952a1-b964-4aa7-8e49-966fa26add54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "168952a1-b964-4aa7-8e49-966fa26add54",
    "outputId": "a0a692c1-6412-4068-8aa5-8858548141eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ae26dd-d77e-41fd-b924-6bd103dd4ee7",
   "metadata": {},
   "source": [
    "- The perplexity is often considered more interpretable because it can be understood as the effective vocabulary size that the model is uncertain about at each step (in the example above, that'd be 47,678 words or tokens)\n",
    "- In other words, perplexity provides a measure of how well the probability distribution predicted by the model matches the actual distribution of the words in the dataset\n",
    "- Similar to the loss, a lower perplexity indicates that the model predictions are closer to the actual distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec6c217-e429-40c7-ad71-5d0a9da8e487",
   "metadata": {
    "id": "2ec6c217-e429-40c7-ad71-5d0a9da8e487"
   },
   "source": [
    "### 5.1.3 Calculating the training and validation set losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530da89e-2448-436c-8f1b-28e8a31ef85c",
   "metadata": {},
   "source": [
    "- We use a relatively small dataset for training the LLM (in fact, only one short story)\n",
    "- The reasons are:\n",
    "  - You can run the code examples in a few minutes on a laptop computer without a suitable GPU\n",
    "  - The training finishes relatively fast (minutes instead of weeks), which is good for educational purposes\n",
    "  - We use a text from the public domain, which can be included in this GitHub repository without violating any usage rights or bloating the repository size\n",
    "\n",
    "\n",
    "- For example, Llama 2 7B required 184,320 GPU hours on A100 GPUs to be trained on 2 trillion tokens\n",
    "  - At the time of this writing, the hourly cost of an 8xA100 cloud server at AWS is approximately \\\\$30\n",
    "  - So, via an off-the-envelope calculation, training this LLM would cost 184,320 / 8 * \\\\$30 =  \\\\$690,000\n",
    " \n",
    "- Below, we use the same dataset we used in chapter 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "654fde37-b2a9-4a20-a8d3-0206c056e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379330f1-80f4-4e34-8724-41d892b04cee",
   "metadata": {},
   "source": [
    "- A quick check that the text loaded ok by printing the first and last 100 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6kgJbe4ehI4q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "6kgJbe4ehI4q",
    "outputId": "9ff31e88-ee37-47e9-ee64-da6eb552f46f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "# First 100 characters\n",
    "print(text_data[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "j2XPde_ThM_e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "j2XPde_ThM_e",
    "outputId": "a900c1b9-9a87-4078-968b-a5721deda5cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n"
     ]
    }
   ],
   "source": [
    "# Last 100 characters\n",
    "print(text_data[-99:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b46a952-d50a-4837-af09-4095698f7fd1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b46a952-d50a-4837-af09-4095698f7fd1",
    "outputId": "c2a25334-21ca-486e-8226-0296e5fc6486"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8830cb9-90f6-4e7c-8620-beeabc2d39f7",
   "metadata": {},
   "source": [
    "- With 5,145 tokens, the text is very short for training an LLM, but again, it's for educational purposes (we will also load pretrained weights later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedcad87-a0e8-4b9d-ac43-4e927ccbb50f",
   "metadata": {},
   "source": [
    "- Next, we divide the dataset into a training and a validation set and use the data loaders from chapter 2 to prepare the batches for LLM training\n",
    "- For visualization purposes, the figure below assumes a `max_length=6`, but for the training loader, we set the `max_length` equal to the context length that the LLM supports\n",
    "- The figure below only shows the input tokens for simplicity\n",
    "    - Since we train the LLM to predict the next word in the text, the targets look the same as these inputs, except that the targets are shifted by one position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bdaa07-ba96-4ac1-9d71-b3cc153910d9",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/batching.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0959c855-f860-4358-8b98-bc654f047578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import create_dataloader_v1\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f37b3eb0-854e-4895-9898-fa7d1e67566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c722add1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I HAD always thought] -> [ HAD always thought ]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "x, y = [(x,y) for (x,y) in train_loader][0]\n",
    "print(f\"[{tokenizer.decode(list(x[0,:]))[:20]}] -> [{tokenizer.decode(list(y[0,:]))[:20]}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ac3296-a4d1-4303-9ac5-376518960c33",
   "metadata": {},
   "source": [
    "- We use a relatively small batch size to reduce the computational resource demand, and because the dataset is very small to begin with\n",
    "- Llama 2 7B was trained with a batch size of 1024, for example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e0514d-b990-4dc0-9afb-7721993284a0",
   "metadata": {},
   "source": [
    "- An optional check that the data was loaded correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca0116d0-d229-472c-9fbf-ebc229331c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "tensor([[22645,    11,   465, 10904,  4252,  6236,   429, 25839,  9230,   808,\n",
      "           276,   416,   257,  8212,   326, 13663,   262,  9040,   286,   257,\n",
      "          2116,    12, 10414,   738,   285, 23968,  4891,    11,   314,  2936,\n",
      "           284,   644,   257,  4922,   339,   550,   262,   976,  3081,   355,\n",
      "           465,  5986,   438,  1169,  3081,   286,  2045,  1190,  4119,    81,\n",
      "           621,   339,   373,    13,   198,   198,  6653,  3656, 27846,   379,\n",
      "           683,  1207,  8344,   803,   306,    11,   475,   465,  2951, 21650,\n",
      "          1613,   607,   284,   262, 18560,    13,   198,   198,     1,  5246,\n",
      "            13,  8759,  2763,  2227,   284,   766,   340,   553,   673,  2540,\n",
      "            11,   355,   611,  2859,  3500,  5223,    13,   679, 28271,   465,\n",
      "         12450,    11,   991, 16755,    13,   198,   198,     1,  5812,    11,\n",
      "          8759,  2763,  1043,   502,   503,   890,  2084,   553,   339,   531,\n",
      "         15376,    26,   788,    11,  6427,   465,  3211,   832,  6164,    25,\n",
      "           366, 16773,   290,   766,   262,  1334,   286,   262,  2156,   526,\n",
      "           198,   198,  1544,  3751,   340,   284,   502,   351,   257,  1611,\n",
      "           286, 24354, 20154, 11293,    25,   262,  7837,    12,  9649,    11,\n",
      "           262,  5486,    12,    83, 29080,    11,   262,  6576,    12,   565,\n",
      "           418,  1039,    11,   262,  4057,  2655,    12,  8439,   274,   438,\n",
      "           439,   262,  3716,  7106,  6637,   286,   262, 45172,   338,  5928,\n",
      "          3773,    13,   843,  8797,   616,  4240,  3432,   262,  2938, 17547,\n",
      "           339,   531,    11,  9644,   503,   465,  7721,   257,  1310,    25,\n",
      "           366,  5297,    11,   314,  1107,   836,   470,   766,   703,   661,\n",
      "          6687,   284,  2107,  1231,   326,   526,   198,   198,  5779,   438,\n",
      "           270,   373,   655,   262,   886,   530,  1244,   423,  1674, 15898,\n",
      "           329,   683,    13,  5514,   339,   373,    11,   832,   340,   477,\n",
      "           290,   287, 15275,   286,   340,   477],\n",
      "        [12036,   683,     0,  3226,  1781,   314,  4001,   284,   466,   262,\n",
      "          4286,   329,  2147,   438,    40,  1297,  9074,    13,   520,  5493,\n",
      "           523,   618,   673,  2540,   284,   336,   321,   647,  1223,   546,\n",
      "           607,  8098,    13,   314,  3505,  1972,   572,   257, 40426, 10956,\n",
      "          9546,   546,   262, 15393,   852,  4808,  3810,    62,   438,  1219,\n",
      "            11,   314,   373, 19716,   306,    11,   616, 13674,  8759,  2763,\n",
      "             0,   314,   373, 24380,   284,  3589,   588,   530,   286,   616,\n",
      "           898,  1650,  1010,    13,   198,   198,     1,  6423,   314,   373,\n",
      "          2077,   510,   290,  1364,  3436,   351,   683,    13,   314,   550,\n",
      "          1908,   477,   616, 20348,   287,  5963,    11,   290,   314,   550,\n",
      "           691,   284,   900,   510,   262,  1396,   417,   290,   651,   284,\n",
      "           670,    13,   679,   550,   587,  2636,   691,  8208,    12, 14337,\n",
      "          2250,    11,   290,   339,  3724,  6451,    11,   286,  2612,  4369,\n",
      "            11,   523,   326,   612,   550,   587,   645, 15223,   670,   286,\n",
      "          8166,   438, 14363,  1986,   373,  1598,   290, 36519,    13,   314,\n",
      "           550,  1138,   683,  1752,   393,  5403,    11,   812,   878,    11,\n",
      "           290,  1807,   683, 32081,   290, 44852,    88,    13,  2735,   314,\n",
      "          2497,   326,   339,   373, 21840,    13,   198,   198,     1,    40,\n",
      "           373,  9675,   379,   717,    11,   351,   257,  6974, 19713, 14676,\n",
      "            25,  9675,   284,   423,   616,  1021,   319,   884,   257,   705,\n",
      "         32796,  2637,  3244,   465,  6283,  1204,    12, 46965,  9449,  2540,\n",
      "           284,  2689,   502, 24506,   306,   438,   292,   314, 10226,   262,\n",
      "          1182,   287,   314,  2936,   355,   611,   339,   547,  4964,   502,\n",
      "           466,   340,    13,   383, 18098,   373,  3940,   416,   262,  1807,\n",
      "            25,   611,   339,  4808, 22474,    62,  4964,   502,    11,   644,\n",
      "           561,   339,   910,   284,   616,   835]]) tensor([[   11,   465, 10904,  4252,  6236,   429, 25839,  9230,   808,   276,\n",
      "           416,   257,  8212,   326, 13663,   262,  9040,   286,   257,  2116,\n",
      "            12, 10414,   738,   285, 23968,  4891,    11,   314,  2936,   284,\n",
      "           644,   257,  4922,   339,   550,   262,   976,  3081,   355,   465,\n",
      "          5986,   438,  1169,  3081,   286,  2045,  1190,  4119,    81,   621,\n",
      "           339,   373,    13,   198,   198,  6653,  3656, 27846,   379,   683,\n",
      "          1207,  8344,   803,   306,    11,   475,   465,  2951, 21650,  1613,\n",
      "           607,   284,   262, 18560,    13,   198,   198,     1,  5246,    13,\n",
      "          8759,  2763,  2227,   284,   766,   340,   553,   673,  2540,    11,\n",
      "           355,   611,  2859,  3500,  5223,    13,   679, 28271,   465, 12450,\n",
      "            11,   991, 16755,    13,   198,   198,     1,  5812,    11,  8759,\n",
      "          2763,  1043,   502,   503,   890,  2084,   553,   339,   531, 15376,\n",
      "            26,   788,    11,  6427,   465,  3211,   832,  6164,    25,   366,\n",
      "         16773,   290,   766,   262,  1334,   286,   262,  2156,   526,   198,\n",
      "           198,  1544,  3751,   340,   284,   502,   351,   257,  1611,   286,\n",
      "         24354, 20154, 11293,    25,   262,  7837,    12,  9649,    11,   262,\n",
      "          5486,    12,    83, 29080,    11,   262,  6576,    12,   565,   418,\n",
      "          1039,    11,   262,  4057,  2655,    12,  8439,   274,   438,   439,\n",
      "           262,  3716,  7106,  6637,   286,   262, 45172,   338,  5928,  3773,\n",
      "            13,   843,  8797,   616,  4240,  3432,   262,  2938, 17547,   339,\n",
      "           531,    11,  9644,   503,   465,  7721,   257,  1310,    25,   366,\n",
      "          5297,    11,   314,  1107,   836,   470,   766,   703,   661,  6687,\n",
      "           284,  2107,  1231,   326,   526,   198,   198,  5779,   438,   270,\n",
      "           373,   655,   262,   886,   530,  1244,   423,  1674, 15898,   329,\n",
      "           683,    13,  5514,   339,   373,    11,   832,   340,   477,   290,\n",
      "           287, 15275,   286,   340,   477,   438],\n",
      "        [  683,     0,  3226,  1781,   314,  4001,   284,   466,   262,  4286,\n",
      "           329,  2147,   438,    40,  1297,  9074,    13,   520,  5493,   523,\n",
      "           618,   673,  2540,   284,   336,   321,   647,  1223,   546,   607,\n",
      "          8098,    13,   314,  3505,  1972,   572,   257, 40426, 10956,  9546,\n",
      "           546,   262, 15393,   852,  4808,  3810,    62,   438,  1219,    11,\n",
      "           314,   373, 19716,   306,    11,   616, 13674,  8759,  2763,     0,\n",
      "           314,   373, 24380,   284,  3589,   588,   530,   286,   616,   898,\n",
      "          1650,  1010,    13,   198,   198,     1,  6423,   314,   373,  2077,\n",
      "           510,   290,  1364,  3436,   351,   683,    13,   314,   550,  1908,\n",
      "           477,   616, 20348,   287,  5963,    11,   290,   314,   550,   691,\n",
      "           284,   900,   510,   262,  1396,   417,   290,   651,   284,   670,\n",
      "            13,   679,   550,   587,  2636,   691,  8208,    12, 14337,  2250,\n",
      "            11,   290,   339,  3724,  6451,    11,   286,  2612,  4369,    11,\n",
      "           523,   326,   612,   550,   587,   645, 15223,   670,   286,  8166,\n",
      "           438, 14363,  1986,   373,  1598,   290, 36519,    13,   314,   550,\n",
      "          1138,   683,  1752,   393,  5403,    11,   812,   878,    11,   290,\n",
      "          1807,   683, 32081,   290, 44852,    88,    13,  2735,   314,  2497,\n",
      "           326,   339,   373, 21840,    13,   198,   198,     1,    40,   373,\n",
      "          9675,   379,   717,    11,   351,   257,  6974, 19713, 14676,    25,\n",
      "          9675,   284,   423,   616,  1021,   319,   884,   257,   705, 32796,\n",
      "          2637,  3244,   465,  6283,  1204,    12, 46965,  9449,  2540,   284,\n",
      "          2689,   502, 24506,   306,   438,   292,   314, 10226,   262,  1182,\n",
      "           287,   314,  2936,   355,   611,   339,   547,  4964,   502,   466,\n",
      "           340,    13,   383, 18098,   373,  3940,   416,   262,  1807,    25,\n",
      "           611,   339,  4808, 22474,    62,  4964,   502,    11,   644,   561,\n",
      "           339,   910,   284,   616,   835,   286]])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "tensor([[  271, 10899,   550,   366,  7109, 14655,   683,   866,   526,  1114,\n",
      "          9074,    13,   402,   271, 10899,   438,   292,   884,   438, 18108,\n",
      "           407, 11196, 10597,  3016,   257,   614,   706,  3619,   338, 10568,\n",
      "           550,   587,  2077,    13,   632,  1244,   307,   326,   339,   550,\n",
      "          6405,   607,   438, 20777,   339,  8288,   465, 10152,   438, 13893,\n",
      "           339,  1422,   470,   765,   284,   467,   319, 12036,    26,   475,\n",
      "           340,   561,   423,   587,  1327,   284,  5879,   326,   339,   550,\n",
      "          1813,   510,   465, 12036,   780,   339,   550,  6405,   607,    13,\n",
      "           198,   198,  5189,  1781,    11,   611,   673,   550,   407, 17901,\n",
      "           683,   866,    11,   673,   550,  8603,    11,   355,  4544,  9325,\n",
      "           701, 42397,    11,  4054,   284,   366, 26282,   683,   510,     1,\n",
      "           438,  7091,   550,   407,  2957,   683,   736,   284,   262,  1396,\n",
      "           417,    13,  1675,  1234,   262, 14093,   656,   465,  1021,   757,\n",
      "           438, 10919,   257,   410,  5040,   329,   257,  3656,     0,   887,\n",
      "          9074,    13,   402,   271, 10899,  4120,   284,   423,   595,    67,\n",
      "          1328,   340,   438,   392,   314,  2936,   340,  1244,   307,  3499,\n",
      "           284,  1064,   503,  1521,    13,   198,   198,   464,   748,   586,\n",
      "           652,  1204,   286,   262, 34686, 41976, 37733,  2346,   284,   884,\n",
      "         14177,  8233,  1020,  5768,    26,   290,  1719,    11,   319,   616,\n",
      "           835,   284, 22489, 40089,    11,  4978,   257, 19350,   286,  3619,\n",
      "           338,  3652,   436,    81,  5286,  8812,  2114,  1022,   262,   279,\n",
      "          1127,    11,   314,   550,  3589, 28068,   294,  1555,   262,  1306,\n",
      "          1110,    13,   198,   198,    40,  1043,   262,  3155,   379,  8887,\n",
      "         11061,   511, 18057,    12,    83,  6037,    26,   290,  9074,    13,\n",
      "           402,   271, 10899,   338,  7062,   373,   523,  2429,   498,   326,\n",
      "            11,   287,   262, 29543,  2745,    11],\n",
      "        [ 1459,   714,  1239,   423,  4499,   326, 18680,   510,    12,  5532,\n",
      "         14000,    13,   764,   764,   764,   198,   198,     1,    40,  2900,\n",
      "           736,   284,   616,   670,    11,   290,  1816,   319, 39136,   278,\n",
      "           290,   285,  4185,  1359,    26,   788,   314,  3114,   379,   262,\n",
      "         50085,   757,    13,   314,  2497,   326,    11,   618,   520,  5493,\n",
      "          8104,   287,   262,   717, 14000,    11,   339,  2993,   655,   644,\n",
      "           262,   886,   561,   307,    13,   679,   550, 17273,   465,  2426,\n",
      "            11, 19233,   340,    11, 11027,   515,   340,    13,  1649,   550,\n",
      "           314,  1760,   326,   351,   597,   286,   616,  1243,    30,  1119,\n",
      "          8020,   470,   587,  4642,   286,   502,   438,    40,   550,   655,\n",
      "          8197,   606,    13,   764,   764,   764,   198,   198,     1,    39,\n",
      "           648,   340,    11,  8759,  2763,    11,   351,   326,  1986,  4964,\n",
      "           502,   314,  3521,   470,   466,  1194, 14000,    13,   383,  8631,\n",
      "          3872,   373,    11,   314,  1422,   470,   760,   810,   284,  1234,\n",
      "           340,   438,    62,    40,   550,  1239,  1900, 44807,  5514,    11,\n",
      "           351,   616,  1650,  1010,   290,   616,  1171,    11,   257,   905,\n",
      "            88, 22870,   286,  9568,  5017,   510,   262,  1109,   438,    40,\n",
      "           655,  9617,  7521,   656,   511,  6698,    13,   764,   764,   764,\n",
      "          3894,    11,  7521,   373,   262,   530,  7090,   883,  2636,  2951,\n",
      "           714,   766,   832,   438,  3826,  3892,   284,   262,  2006, 20212,\n",
      "         19369, 14638,    13,  2094,   470,   345,   760,   703,    11,   287,\n",
      "          3375,   257,  3215,  3303,    11,   772,  6562,  1473,    11,   530,\n",
      "          1139,  2063,   262,   640,   407,   644,   530,  3382,   284,   475,\n",
      "           644,   530,   460,    30,  3894,   438,  5562,   373,   262,   835,\n",
      "           314, 13055,    26,   290,   355,   339,  3830,   612,   290,  7342,\n",
      "           502,    11,   262,  1517,   484,  1444]]) tensor([[10899,   550,   366,  7109, 14655,   683,   866,   526,  1114,  9074,\n",
      "            13,   402,   271, 10899,   438,   292,   884,   438, 18108,   407,\n",
      "         11196, 10597,  3016,   257,   614,   706,  3619,   338, 10568,   550,\n",
      "           587,  2077,    13,   632,  1244,   307,   326,   339,   550,  6405,\n",
      "           607,   438, 20777,   339,  8288,   465, 10152,   438, 13893,   339,\n",
      "          1422,   470,   765,   284,   467,   319, 12036,    26,   475,   340,\n",
      "           561,   423,   587,  1327,   284,  5879,   326,   339,   550,  1813,\n",
      "           510,   465, 12036,   780,   339,   550,  6405,   607,    13,   198,\n",
      "           198,  5189,  1781,    11,   611,   673,   550,   407, 17901,   683,\n",
      "           866,    11,   673,   550,  8603,    11,   355,  4544,  9325,   701,\n",
      "         42397,    11,  4054,   284,   366, 26282,   683,   510,     1,   438,\n",
      "          7091,   550,   407,  2957,   683,   736,   284,   262,  1396,   417,\n",
      "            13,  1675,  1234,   262, 14093,   656,   465,  1021,   757,   438,\n",
      "         10919,   257,   410,  5040,   329,   257,  3656,     0,   887,  9074,\n",
      "            13,   402,   271, 10899,  4120,   284,   423,   595,    67,  1328,\n",
      "           340,   438,   392,   314,  2936,   340,  1244,   307,  3499,   284,\n",
      "          1064,   503,  1521,    13,   198,   198,   464,   748,   586,   652,\n",
      "          1204,   286,   262, 34686, 41976, 37733,  2346,   284,   884, 14177,\n",
      "          8233,  1020,  5768,    26,   290,  1719,    11,   319,   616,   835,\n",
      "           284, 22489, 40089,    11,  4978,   257, 19350,   286,  3619,   338,\n",
      "          3652,   436,    81,  5286,  8812,  2114,  1022,   262,   279,  1127,\n",
      "            11,   314,   550,  3589, 28068,   294,  1555,   262,  1306,  1110,\n",
      "            13,   198,   198,    40,  1043,   262,  3155,   379,  8887, 11061,\n",
      "           511, 18057,    12,    83,  6037,    26,   290,  9074,    13,   402,\n",
      "           271, 10899,   338,  7062,   373,   523,  2429,   498,   326,    11,\n",
      "           287,   262, 29543,  2745,    11,   314],\n",
      "        [  714,  1239,   423,  4499,   326, 18680,   510,    12,  5532, 14000,\n",
      "            13,   764,   764,   764,   198,   198,     1,    40,  2900,   736,\n",
      "           284,   616,   670,    11,   290,  1816,   319, 39136,   278,   290,\n",
      "           285,  4185,  1359,    26,   788,   314,  3114,   379,   262, 50085,\n",
      "           757,    13,   314,  2497,   326,    11,   618,   520,  5493,  8104,\n",
      "           287,   262,   717, 14000,    11,   339,  2993,   655,   644,   262,\n",
      "           886,   561,   307,    13,   679,   550, 17273,   465,  2426,    11,\n",
      "         19233,   340,    11, 11027,   515,   340,    13,  1649,   550,   314,\n",
      "          1760,   326,   351,   597,   286,   616,  1243,    30,  1119,  8020,\n",
      "           470,   587,  4642,   286,   502,   438,    40,   550,   655,  8197,\n",
      "           606,    13,   764,   764,   764,   198,   198,     1,    39,   648,\n",
      "           340,    11,  8759,  2763,    11,   351,   326,  1986,  4964,   502,\n",
      "           314,  3521,   470,   466,  1194, 14000,    13,   383,  8631,  3872,\n",
      "           373,    11,   314,  1422,   470,   760,   810,   284,  1234,   340,\n",
      "           438,    62,    40,   550,  1239,  1900, 44807,  5514,    11,   351,\n",
      "           616,  1650,  1010,   290,   616,  1171,    11,   257,   905,    88,\n",
      "         22870,   286,  9568,  5017,   510,   262,  1109,   438,    40,   655,\n",
      "          9617,  7521,   656,   511,  6698,    13,   764,   764,   764,  3894,\n",
      "            11,  7521,   373,   262,   530,  7090,   883,  2636,  2951,   714,\n",
      "           766,   832,   438,  3826,  3892,   284,   262,  2006, 20212, 19369,\n",
      "         14638,    13,  2094,   470,   345,   760,   703,    11,   287,  3375,\n",
      "           257,  3215,  3303,    11,   772,  6562,  1473,    11,   530,  1139,\n",
      "          2063,   262,   640,   407,   644,   530,  3382,   284,   475,   644,\n",
      "           530,   460,    30,  3894,   438,  5562,   373,   262,   835,   314,\n",
      "         13055,    26,   290,   355,   339,  3830,   612,   290,  7342,   502,\n",
      "            11,   262,  1517,   484,  1444,   616]])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "tensor([[  198,  1544, 13818,  4622,    11,  1231, 35987,    11,   290,  7121,\n",
      "           530,   286,   262,  2769,  3211,    12, 49655,  2651,    13,   366,\n",
      "          1858,    25,   787,  3511,  6792,   438,   392,   994,   389,   262,\n",
      "         33204,   345,   588,   526,   198,   198,  1544,  4624,   606,   379,\n",
      "           616, 22662,   290,  3767,   284, 27776,   510,   290,   866,   262,\n",
      "          2119,    11, 12225,   783,   290,   788, 11061,   262,  4286,    13,\n",
      "           198,   198,     1,  2437,   340,  3022,    30,   314,   460,  1560,\n",
      "           345,   287,  1936,  2431,   438,   392,   340,  1422,   470,  1011,\n",
      "           881,  2392,   284,  1645,    13,   764,   764,   764,   314,   460,\n",
      "          3505,   783,   703,  6655,   290, 10607,   314,   373,   618,   314,\n",
      "          1392,  9074,    13,   520,  5493,   338,  3465,    13,  3226,  1781,\n",
      "            11,  2769,   866,    11,   314,   550,  1464,  4808, 31985,    62,\n",
      "           612,   373,   645,   530,   588,   683,   438,  8807,   314,   550,\n",
      "          3750,   351,   262,  4269,    11, 22211,   262,  6678, 40315, 10455,\n",
      "           546,   683,    11, 10597,   314,  2063,  1392,   284,   892,   339,\n",
      "           373,   257,  5287,    11,   530,   286,   262,  1611,   326,   389,\n",
      "          1364,  2157,    13,  2750,   449,   659,    11,   290,   339,  4808,\n",
      "          9776,    62,  1364,  2157,   438, 13893,   339,   550,  1282,   284,\n",
      "          2652,     0,   383,  1334,   286,   514,   550,   284,  1309,  6731,\n",
      "           307, 17676,  1863,   393,   467,   739,    11,   475,   339,   373,\n",
      "          1029,  2029,   262,  1459,   438,   261, 45697, 19369,    11,   355,\n",
      "           345,   910,    13,   198,   198,     1,  5779,    11,   314,  1816,\n",
      "           572,   284,   262,  2156,   287,   616,   749, 34372, 10038,   438,\n",
      "         34330,  3888,    11,  4453, 20927,   502,    11,   379,   262,  3108,\n",
      "           418,   286,  3595,   520,  5493,   338,  3451,   286,  5287,   852,\n",
      "         37492,   416,   262, 13476,   286,   616],\n",
      "        [  286,  1762,    30,  2011, 29483,  2540,   284,   467,   257,  1310,\n",
      "          4295,   438,    40,  2936, 10927,   290,  8627,    13,   198,   198,\n",
      "             1,  7454,    11,   618,   314,  3114,   510,    11,   314,  3947,\n",
      "           284,   766,   257,  8212,  2157,   465,  1969, 12768,   680, 21213,\n",
      "           438,   292,   611,   339,   550,   262,  3200,    11,   290,   547,\n",
      "         28297,  2241,   416,  4769,   340,   736,   422,   502,    13,  1320,\n",
      "         41851,   515,   502,   991,   517,    13,   383,  3200,    30,  4162,\n",
      "            11,   314,   550,   257,  3200,  2861,  8208,   286,   465,     0,\n",
      "           314, 37901,   379,   262, 21978, 44896,    11,   290,  3088,   617,\n",
      "           286,   616, 49025,  5330, 15910,    13,   887,   484,  4054,   502,\n",
      "            11,   484,  1067, 11137,    13,   314,  2497,   326,   339,  2492,\n",
      "           470,  4964,   262,   905,    88, 10340,   438,    40,  3521,   470,\n",
      "         11786,   465,  3241,    26,   339,   655,  4030,   465,  2951,   319,\n",
      "           262,  1327, 22674,  1022,    13,  5845,   547,   262,  3392,   314,\n",
      "           550,  1464,   427,   343,  9091,    11,   393,  5017,   510,   351,\n",
      "           617,  9105,  7521,    13,   843,   703,   339,  2497,   832,   616,\n",
      "          7363,     0,   198,   198,     1,    40,  3114,   510,   757,    11,\n",
      "           290,  4978,  6504,   286,   326, 17548,   286,   262, 50085, 10938,\n",
      "           319,   262,  3355,  1474,   465,  3996,    13,  2399,  3656,  1297,\n",
      "           502, 20875,   340,   373,   262,   938,  1517,   339,   550,  1760,\n",
      "           438,  3137,   257,  3465,  2077,   351,   257, 17275,  1021,    11,\n",
      "           618,   339,   373,   866,   287,  6245,   684, 10695, 20222,   422,\n",
      "           257,  2180,  2612,  1368,    13,  2329,   257,  3465,     0,   887,\n",
      "           340,  4952,   465,  2187,  2106,    13,  1318,   389,   812,   286,\n",
      "          5827, 40987,   913, 30802,   287,   790,  1627,    13,   317,   582,\n",
      "           508,   550,  1509,   388,   351,   262]]) tensor([[ 1544, 13818,  4622,    11,  1231, 35987,    11,   290,  7121,   530,\n",
      "           286,   262,  2769,  3211,    12, 49655,  2651,    13,   366,  1858,\n",
      "            25,   787,  3511,  6792,   438,   392,   994,   389,   262, 33204,\n",
      "           345,   588,   526,   198,   198,  1544,  4624,   606,   379,   616,\n",
      "         22662,   290,  3767,   284, 27776,   510,   290,   866,   262,  2119,\n",
      "            11, 12225,   783,   290,   788, 11061,   262,  4286,    13,   198,\n",
      "           198,     1,  2437,   340,  3022,    30,   314,   460,  1560,   345,\n",
      "           287,  1936,  2431,   438,   392,   340,  1422,   470,  1011,   881,\n",
      "          2392,   284,  1645,    13,   764,   764,   764,   314,   460,  3505,\n",
      "           783,   703,  6655,   290, 10607,   314,   373,   618,   314,  1392,\n",
      "          9074,    13,   520,  5493,   338,  3465,    13,  3226,  1781,    11,\n",
      "          2769,   866,    11,   314,   550,  1464,  4808, 31985,    62,   612,\n",
      "           373,   645,   530,   588,   683,   438,  8807,   314,   550,  3750,\n",
      "           351,   262,  4269,    11, 22211,   262,  6678, 40315, 10455,   546,\n",
      "           683,    11, 10597,   314,  2063,  1392,   284,   892,   339,   373,\n",
      "           257,  5287,    11,   530,   286,   262,  1611,   326,   389,  1364,\n",
      "          2157,    13,  2750,   449,   659,    11,   290,   339,  4808,  9776,\n",
      "            62,  1364,  2157,   438, 13893,   339,   550,  1282,   284,  2652,\n",
      "             0,   383,  1334,   286,   514,   550,   284,  1309,  6731,   307,\n",
      "         17676,  1863,   393,   467,   739,    11,   475,   339,   373,  1029,\n",
      "          2029,   262,  1459,   438,   261, 45697, 19369,    11,   355,   345,\n",
      "           910,    13,   198,   198,     1,  5779,    11,   314,  1816,   572,\n",
      "           284,   262,  2156,   287,   616,   749, 34372, 10038,   438, 34330,\n",
      "          3888,    11,  4453, 20927,   502,    11,   379,   262,  3108,   418,\n",
      "           286,  3595,   520,  5493,   338,  3451,   286,  5287,   852, 37492,\n",
      "           416,   262, 13476,   286,   616, 12036],\n",
      "        [ 1762,    30,  2011, 29483,  2540,   284,   467,   257,  1310,  4295,\n",
      "           438,    40,  2936, 10927,   290,  8627,    13,   198,   198,     1,\n",
      "          7454,    11,   618,   314,  3114,   510,    11,   314,  3947,   284,\n",
      "           766,   257,  8212,  2157,   465,  1969, 12768,   680, 21213,   438,\n",
      "           292,   611,   339,   550,   262,  3200,    11,   290,   547, 28297,\n",
      "          2241,   416,  4769,   340,   736,   422,   502,    13,  1320, 41851,\n",
      "           515,   502,   991,   517,    13,   383,  3200,    30,  4162,    11,\n",
      "           314,   550,   257,  3200,  2861,  8208,   286,   465,     0,   314,\n",
      "         37901,   379,   262, 21978, 44896,    11,   290,  3088,   617,   286,\n",
      "           616, 49025,  5330, 15910,    13,   887,   484,  4054,   502,    11,\n",
      "           484,  1067, 11137,    13,   314,  2497,   326,   339,  2492,   470,\n",
      "          4964,   262,   905,    88, 10340,   438,    40,  3521,   470, 11786,\n",
      "           465,  3241,    26,   339,   655,  4030,   465,  2951,   319,   262,\n",
      "          1327, 22674,  1022,    13,  5845,   547,   262,  3392,   314,   550,\n",
      "          1464,   427,   343,  9091,    11,   393,  5017,   510,   351,   617,\n",
      "          9105,  7521,    13,   843,   703,   339,  2497,   832,   616,  7363,\n",
      "             0,   198,   198,     1,    40,  3114,   510,   757,    11,   290,\n",
      "          4978,  6504,   286,   326, 17548,   286,   262, 50085, 10938,   319,\n",
      "           262,  3355,  1474,   465,  3996,    13,  2399,  3656,  1297,   502,\n",
      "         20875,   340,   373,   262,   938,  1517,   339,   550,  1760,   438,\n",
      "          3137,   257,  3465,  2077,   351,   257, 17275,  1021,    11,   618,\n",
      "           339,   373,   866,   287,  6245,   684, 10695, 20222,   422,   257,\n",
      "          2180,  2612,  1368,    13,  2329,   257,  3465,     0,   887,   340,\n",
      "          4952,   465,  2187,  2106,    13,  1318,   389,   812,   286,  5827,\n",
      "         40987,   913, 30802,   287,   790,  1627,    13,   317,   582,   508,\n",
      "           550,  1509,   388,   351,   262,  1459]])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "tensor([[   11, 17728,   257,  8500,  4417,   284,   670,   319,   438, 15464,\n",
      "            11,   355,   340,   547,    11,   523, 16857,   262,  4469,   286,\n",
      "           607,   898,  4286,   438, 18108, 26269,  5223,   287,   281,  8468,\n",
      "          4922,   284,   262,  3359,   286,   428,  3991,  4118,    84, 16579,\n",
      "            13,   383,  4286,   373,   530,   286,  3619,   338,   366, 11576,\n",
      "           395,   553,   355,   465, 21099,  3808,   561,   423,  1234,   340,\n",
      "           438,   270,  7997,    11,   319,   465,   636,    11,   257, 29844,\n",
      "           286, 12749,    11,   257, 22791,   278,   286, 32375,    11,   257,\n",
      "         22486,    11,   965,  2860,  1359,   290,   965,  1397,    11,   326,\n",
      "         14516,   530,   286,   262, 33125,    12,   565,   593,   338, 25304,\n",
      "          4040,   284, 10303,   257, 17972,    13,   632,  1138,    11,   287,\n",
      "          1790,    11,   379,   790,   966,   262,  3512,   286, 14081,  2415,\n",
      "           284,   307, 13055,   366, 11576,   306,     1,   780,   673,   373,\n",
      "         10032,   286,   852, 13055,   366, 34751,   306,     1,   438,   392,\n",
      "          1865,   407,   284,  4425,   281, 22037,   286,   262, 32073,    13,\n",
      "           198,   198,     1,  1026,   338,   262,   938,   339, 13055,    11,\n",
      "           345,   760,   553,  9074,    13,   402,   271, 10899,   531,   351,\n",
      "         27322,   540, 11293,    13,   366,   464,   938,   475,   530,   553,\n",
      "           673, 19267,  5223,   438,     1,  4360,   262,   584,  1595,   470,\n",
      "           954,    11,   780,   339,  6572,   340,   526,   198,   198,     1,\n",
      "         49174,   276,   340,  1701,   314,   373,   546,   284,  1061,   510,\n",
      "           428, 18437,   618,   314,  2982,   257,  2366,  9662,   290,  2497,\n",
      "          3619,  2241,   319,   262, 11387,    13,   198,   198,  1722,   339,\n",
      "          6204,   612,    11,   465,  2832,   287,   262, 16511,   286,   465,\n",
      "         11555,   303,  7821, 13209,    11,   262,  7888,  7586,  9813,   286,\n",
      "          4190,  7121,   736,   422,   465,  2330],\n",
      "        [   13,   198,   198,     1, 19242,   339,   442, 17758,   465,  5986,\n",
      "          1165,    30,   314,  4398,   470,  1775,   257,  2060,   530,   287,\n",
      "           262,  2156,   526,   198,   198,    32,  3731, 17979,   286, 32315,\n",
      "         12606,  9074,    13,   402,   271, 10899,   338,  1280,   954, 36368,\n",
      "            13,   366,  1026,   338,   465, 11441, 48740,    11,   345,   760,\n",
      "            13,   679,  1139,   484,   821,   407,  4197,   284,   423,   546,\n",
      "            26,   339,   338,  1908,   606,   477,  1497,  2845,   530,   438,\n",
      "          1820, 18560,   438,   392,   326,   314,   423,   284,  1394, 26148,\n",
      "           526,   198,   198,  6653, 11441, 48740,   438, 14295,   338, 48740,\n",
      "           546,   465,  5986,    30,  2011, 20136,   373,  3957,   588,   262,\n",
      "         26394,    12,   301,   971,    13,   314,   531, 10722,   292,  2280,\n",
      "           284,   616,  2583,   408,    25,   366,    40,  1276,  1107,   766,\n",
      "           534, 18560,    11,   345,   760,   526,   198,   198,  3347, 27846,\n",
      "           503,  2048,  4628, 24882,   379,   262,  8812,   558,   810,   607,\n",
      "          5229,    11, 21081,   782,   278,   287,   257, 14263,   276,  5118,\n",
      "            11,   550,  6578,   257, 24518,   290,  7428,   262,  3394, 20096,\n",
      "         39047,   338,  1182,  1022,   465, 14475,    13,   198,   198,     1,\n",
      "          5779,    11,  1282,   981,   339,   338,   407,  2045,   553,   673,\n",
      "           531,    11,   351,   257,  6487,   326,  3088,   284,  7808,   607,\n",
      "         10927,  1108,    26,   290,   314,  3940,   607,  1022,   262, 30623,\n",
      "          2295, 49406,   286,   262,  6899,    11,   290,   510,   262,  3094,\n",
      "         16046,   351,  1059,   430,    12,    66, 12375,   299, 20896,    82,\n",
      "         24357,  1871, 12734,   379,  1123,  9581,    13,   198,   198,   818,\n",
      "           262,  5391,    76,   395,  5228,   286,   607,   275,  2778, 10840,\n",
      "            11, 10371,   257,  1534,  4241,   286, 19217,   290, 18876,  5563,\n",
      "            11,  9174,   530,   286,   262,  5385]]) tensor([[17728,   257,  8500,  4417,   284,   670,   319,   438, 15464,    11,\n",
      "           355,   340,   547,    11,   523, 16857,   262,  4469,   286,   607,\n",
      "           898,  4286,   438, 18108, 26269,  5223,   287,   281,  8468,  4922,\n",
      "           284,   262,  3359,   286,   428,  3991,  4118,    84, 16579,    13,\n",
      "           383,  4286,   373,   530,   286,  3619,   338,   366, 11576,   395,\n",
      "           553,   355,   465, 21099,  3808,   561,   423,  1234,   340,   438,\n",
      "           270,  7997,    11,   319,   465,   636,    11,   257, 29844,   286,\n",
      "         12749,    11,   257, 22791,   278,   286, 32375,    11,   257, 22486,\n",
      "            11,   965,  2860,  1359,   290,   965,  1397,    11,   326, 14516,\n",
      "           530,   286,   262, 33125,    12,   565,   593,   338, 25304,  4040,\n",
      "           284, 10303,   257, 17972,    13,   632,  1138,    11,   287,  1790,\n",
      "            11,   379,   790,   966,   262,  3512,   286, 14081,  2415,   284,\n",
      "           307, 13055,   366, 11576,   306,     1,   780,   673,   373, 10032,\n",
      "           286,   852, 13055,   366, 34751,   306,     1,   438,   392,  1865,\n",
      "           407,   284,  4425,   281, 22037,   286,   262, 32073,    13,   198,\n",
      "           198,     1,  1026,   338,   262,   938,   339, 13055,    11,   345,\n",
      "           760,   553,  9074,    13,   402,   271, 10899,   531,   351, 27322,\n",
      "           540, 11293,    13,   366,   464,   938,   475,   530,   553,   673,\n",
      "         19267,  5223,   438,     1,  4360,   262,   584,  1595,   470,   954,\n",
      "            11,   780,   339,  6572,   340,   526,   198,   198,     1, 49174,\n",
      "           276,   340,  1701,   314,   373,   546,   284,  1061,   510,   428,\n",
      "         18437,   618,   314,  2982,   257,  2366,  9662,   290,  2497,  3619,\n",
      "          2241,   319,   262, 11387,    13,   198,   198,  1722,   339,  6204,\n",
      "           612,    11,   465,  2832,   287,   262, 16511,   286,   465, 11555,\n",
      "           303,  7821, 13209,    11,   262,  7888,  7586,  9813,   286,  4190,\n",
      "          7121,   736,   422,   465,  2330, 22645],\n",
      "        [  198,   198,     1, 19242,   339,   442, 17758,   465,  5986,  1165,\n",
      "            30,   314,  4398,   470,  1775,   257,  2060,   530,   287,   262,\n",
      "          2156,   526,   198,   198,    32,  3731, 17979,   286, 32315, 12606,\n",
      "          9074,    13,   402,   271, 10899,   338,  1280,   954, 36368,    13,\n",
      "           366,  1026,   338,   465, 11441, 48740,    11,   345,   760,    13,\n",
      "           679,  1139,   484,   821,   407,  4197,   284,   423,   546,    26,\n",
      "           339,   338,  1908,   606,   477,  1497,  2845,   530,   438,  1820,\n",
      "         18560,   438,   392,   326,   314,   423,   284,  1394, 26148,   526,\n",
      "           198,   198,  6653, 11441, 48740,   438, 14295,   338, 48740,   546,\n",
      "           465,  5986,    30,  2011, 20136,   373,  3957,   588,   262, 26394,\n",
      "            12,   301,   971,    13,   314,   531, 10722,   292,  2280,   284,\n",
      "           616,  2583,   408,    25,   366,    40,  1276,  1107,   766,   534,\n",
      "         18560,    11,   345,   760,   526,   198,   198,  3347, 27846,   503,\n",
      "          2048,  4628, 24882,   379,   262,  8812,   558,   810,   607,  5229,\n",
      "            11, 21081,   782,   278,   287,   257, 14263,   276,  5118,    11,\n",
      "           550,  6578,   257, 24518,   290,  7428,   262,  3394, 20096, 39047,\n",
      "           338,  1182,  1022,   465, 14475,    13,   198,   198,     1,  5779,\n",
      "            11,  1282,   981,   339,   338,   407,  2045,   553,   673,   531,\n",
      "            11,   351,   257,  6487,   326,  3088,   284,  7808,   607, 10927,\n",
      "          1108,    26,   290,   314,  3940,   607,  1022,   262, 30623,  2295,\n",
      "         49406,   286,   262,  6899,    11,   290,   510,   262,  3094, 16046,\n",
      "           351,  1059,   430,    12,    66, 12375,   299, 20896,    82, 24357,\n",
      "          1871, 12734,   379,  1123,  9581,    13,   198,   198,   818,   262,\n",
      "          5391,    76,   395,  5228,   286,   607,   275,  2778, 10840,    11,\n",
      "         10371,   257,  1534,  4241,   286, 19217,   290, 18876,  5563,    11,\n",
      "          9174,   530,   286,   262,  5385, 41186]])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "tensor([[10197,   832,   262, 46475,   286, 18113,   544,   338, 10953,   314,\n",
      "          2936,  1498,   284,  1986,   262,  1109,   351,  1602, 11227,   414,\n",
      "            13, 23676,  3619,   402,   271, 10899,     0,   383,  1466,   550,\n",
      "           925,   683,   438,   270,   373, 15830,   326,   484,   815, 25722,\n",
      "           683,    13,  9754,   465,   898,  1714,  7380, 30090,   547,  2982,\n",
      "            11,   290,   287,   465,   898,  3292,  8941,   257,  4636, 28582,\n",
      "            13, 18612, 35394,    30,  8673,    13,  1002,   340,   547,    11,\n",
      "           262, 15393,   286,   262,  5977,   373, 29178,  3474,   416,  1310,\n",
      "         40559, 11959,  1636,    11,   508,    11,   287,   477,   922,  4562,\n",
      "            11,  3181,   503,   287,   262, 37090,   257,   845, 22665,   366,\n",
      "           672,   270,  2838,     1,   319,  3619,   438,   505,   286,   883,\n",
      "           905,    88,  6685, 42070,   351,  4738,  6276,   871,   326,   314,\n",
      "           423,  2982,   357,    40,  1839,   470,   910,   416,  4150,     8,\n",
      "          3688,   284,   402,   271, 10899,   338, 12036,    13,   843,   523,\n",
      "           438, 14363, 10568,   852,  5729, 11331, 18893,   540,   438,  1169,\n",
      "          5114, 11835,  3724,   503,    11,   290,    11,   355,  9074,    13,\n",
      "           536,  5469,   550, 11001,    11,   262,  2756,   286,   366,    38,\n",
      "           271, 10899,    82,     1,  1816,   510,    13,   198,   198,  1026,\n",
      "           373,   407, 10597,  1115,   812,  1568,   326,    11,   287,   262,\n",
      "          1781,   286,   257,  1178,  2745,     6,  4686,  1359,   319,   262,\n",
      "         34686, 41976,    11,   340,  6451,  5091,   284,   502,   284,  4240,\n",
      "          1521,   402,   271, 10899,   550,  1813,   510,   465, 12036,    13,\n",
      "          1550, 14580,    11,   340,  1107,   373,   257, 29850,  1917,    13,\n",
      "          1675, 24456,   465,  3656,   561,   423,   587,  1165,  2562,   438,\n",
      "         14363,  3148,  1650,  1010,   550,   587,  6699,   262,  1540,   558,\n",
      "           286,  2282,   326,  9074,    13,   402],\n",
      "        [ 1092,   517,   621,   611,   314,  1549,  1239, 12615,   257, 14093,\n",
      "           526,   198,   198,  1870,   465,  8216,  1297,   502,   287,   257,\n",
      "          7644,   326,   339,  1239,  1807,   286,  1997,  2073,    13,   198,\n",
      "           198,    40,  3888,  1497,    11, 43045, 21100,   416,   616, 10059,\n",
      "          9412,    26,   290,   355,   314,  2900,    11,   616,  4151,  3214,\n",
      "           319,   257,  1402,  4286,  2029,   262, 24818,   417,    12, 12239,\n",
      "           438,  1169,   691,  2134,  7163,   262,  8631, 26210,  3425,  9417,\n",
      "           286,   262,  2119,    13,   198,   198,     1,  5812,    11,   416,\n",
      "           449,   659,  2474,   314,   531,    13,   198,   198,  1026,   373,\n",
      "           257, 17548,   286,   257, 50085,   438,   272,  1468, 10032, 50085,\n",
      "            11,  5055,   287,   262,  6290,   739,   257,  3355,    13,   198,\n",
      "           198,     1,  3886,   449,   659,   438,    64,   520,  5493,  2474,\n",
      "           314, 16896,    13,   198,   198,  1544,   373, 10574,    26,   475,\n",
      "           314,  2936,   683,  1969,  2157,   502,    11, 12704,   257,  1310,\n",
      "          2952,    13,   198,   198,     1,  2061,   257,  4240,     0, 14446,\n",
      "           351,   257,  8667,  3951,   438,  4360,   319, 45697, 19369,    13,\n",
      "           921,  9670, 28022,    11,   810,   750,   345,   651,   340,  1701,\n",
      "           198,   198,  1544,  9373,  6364,    25,   366, 27034,    13,   520,\n",
      "          5493,  2921,   340,   284,   502,   526,   198,   198,     1, 10910,\n",
      "           438,    40,  1422,   470,   760,   345,   772,  2993,   262,   520,\n",
      "          5493,    82,    13,   679,   373,   884,   281,  1167,  2588,   856,\n",
      "           607,  2781,   526,   198,   198,     1,    40,  1422,   470,   438,\n",
      "            83,   359,   706,    13,   764,   764,   764,  1375,  1908,   329,\n",
      "           502,   284,  7521,   683,   618,   339,   373,  2636,   526,   198,\n",
      "           198,     1,  2215,   339,   373,  2636,    30,   921,  1701,   198,\n",
      "           198,    40,  1276,   423,  1309,   257]]) tensor([[  832,   262, 46475,   286, 18113,   544,   338, 10953,   314,  2936,\n",
      "          1498,   284,  1986,   262,  1109,   351,  1602, 11227,   414,    13,\n",
      "         23676,  3619,   402,   271, 10899,     0,   383,  1466,   550,   925,\n",
      "           683,   438,   270,   373, 15830,   326,   484,   815, 25722,   683,\n",
      "            13,  9754,   465,   898,  1714,  7380, 30090,   547,  2982,    11,\n",
      "           290,   287,   465,   898,  3292,  8941,   257,  4636, 28582,    13,\n",
      "         18612, 35394,    30,  8673,    13,  1002,   340,   547,    11,   262,\n",
      "         15393,   286,   262,  5977,   373, 29178,  3474,   416,  1310, 40559,\n",
      "         11959,  1636,    11,   508,    11,   287,   477,   922,  4562,    11,\n",
      "          3181,   503,   287,   262, 37090,   257,   845, 22665,   366,   672,\n",
      "           270,  2838,     1,   319,  3619,   438,   505,   286,   883,   905,\n",
      "            88,  6685, 42070,   351,  4738,  6276,   871,   326,   314,   423,\n",
      "          2982,   357,    40,  1839,   470,   910,   416,  4150,     8,  3688,\n",
      "           284,   402,   271, 10899,   338, 12036,    13,   843,   523,   438,\n",
      "         14363, 10568,   852,  5729, 11331, 18893,   540,   438,  1169,  5114,\n",
      "         11835,  3724,   503,    11,   290,    11,   355,  9074,    13,   536,\n",
      "          5469,   550, 11001,    11,   262,  2756,   286,   366,    38,   271,\n",
      "         10899,    82,     1,  1816,   510,    13,   198,   198,  1026,   373,\n",
      "           407, 10597,  1115,   812,  1568,   326,    11,   287,   262,  1781,\n",
      "           286,   257,  1178,  2745,     6,  4686,  1359,   319,   262, 34686,\n",
      "         41976,    11,   340,  6451,  5091,   284,   502,   284,  4240,  1521,\n",
      "           402,   271, 10899,   550,  1813,   510,   465, 12036,    13,  1550,\n",
      "         14580,    11,   340,  1107,   373,   257, 29850,  1917,    13,  1675,\n",
      "         24456,   465,  3656,   561,   423,   587,  1165,  2562,   438, 14363,\n",
      "          3148,  1650,  1010,   550,   587,  6699,   262,  1540,   558,   286,\n",
      "          2282,   326,  9074,    13,   402,   271],\n",
      "        [  517,   621,   611,   314,  1549,  1239, 12615,   257, 14093,   526,\n",
      "           198,   198,  1870,   465,  8216,  1297,   502,   287,   257,  7644,\n",
      "           326,   339,  1239,  1807,   286,  1997,  2073,    13,   198,   198,\n",
      "            40,  3888,  1497,    11, 43045, 21100,   416,   616, 10059,  9412,\n",
      "            26,   290,   355,   314,  2900,    11,   616,  4151,  3214,   319,\n",
      "           257,  1402,  4286,  2029,   262, 24818,   417,    12, 12239,   438,\n",
      "          1169,   691,  2134,  7163,   262,  8631, 26210,  3425,  9417,   286,\n",
      "           262,  2119,    13,   198,   198,     1,  5812,    11,   416,   449,\n",
      "           659,  2474,   314,   531,    13,   198,   198,  1026,   373,   257,\n",
      "         17548,   286,   257, 50085,   438,   272,  1468, 10032, 50085,    11,\n",
      "          5055,   287,   262,  6290,   739,   257,  3355,    13,   198,   198,\n",
      "             1,  3886,   449,   659,   438,    64,   520,  5493,  2474,   314,\n",
      "         16896,    13,   198,   198,  1544,   373, 10574,    26,   475,   314,\n",
      "          2936,   683,  1969,  2157,   502,    11, 12704,   257,  1310,  2952,\n",
      "            13,   198,   198,     1,  2061,   257,  4240,     0, 14446,   351,\n",
      "           257,  8667,  3951,   438,  4360,   319, 45697, 19369,    13,   921,\n",
      "          9670, 28022,    11,   810,   750,   345,   651,   340,  1701,   198,\n",
      "           198,  1544,  9373,  6364,    25,   366, 27034,    13,   520,  5493,\n",
      "          2921,   340,   284,   502,   526,   198,   198,     1, 10910,   438,\n",
      "            40,  1422,   470,   760,   345,   772,  2993,   262,   520,  5493,\n",
      "            82,    13,   679,   373,   884,   281,  1167,  2588,   856,   607,\n",
      "          2781,   526,   198,   198,     1,    40,  1422,   470,   438,    83,\n",
      "           359,   706,    13,   764,   764,   764,  1375,  1908,   329,   502,\n",
      "           284,  7521,   683,   618,   339,   373,  2636,   526,   198,   198,\n",
      "             1,  2215,   339,   373,  2636,    30,   921,  1701,   198,   198,\n",
      "            40,  1276,   423,  1309,   257,  1310]])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "tensor([[  284,  1234,  8737,   656, 19133,   553,   373,   530,   286,   262,\n",
      "          7877,    72,  3150,   339,  8104,   866,  1973,   262, 37918,   411,\n",
      "           290,  8465,   286,   281, 33954,   271,  3973,  9899, 14678, 40556,\n",
      "            12, 11487,    11,   618,    11,   319,   257,  1568,  1110,    11,\n",
      "           314,   550,   757,  1057,   625,   422, 22489, 40089,    26,   290,\n",
      "          9074,    13,   402,   271, 10899,    11,   307,  3723,   319,   683,\n",
      "            11,  2087,   329,   616, 35957,    25,   366, 14295,   318,   523,\n",
      "         34813,   306,  8564,   284,   790,  1296,   286,  8737,   526,   198,\n",
      "           198, 43920,  3619,     0,   632,   550,  1464,   587,   465, 10030,\n",
      "           284,   423,  1466,   910,   884,  1243,   286,   683,    25,   262,\n",
      "          1109,   815,   307,   900,   866,   287,  1070,   268,  2288,    13,\n",
      "          1867,  7425,   502,   783,   373,   326,    11,   329,   262,   717,\n",
      "           640,    11,   339,   581,  4714,   262,  8216,    13,   314,   550,\n",
      "          1775,   683,    11,   523,  1690,    11,  1615,  3364,   739,  2092,\n",
      "           256,  7657,   438,  9776,   340,   262, 11644, 43778,  3465,   326,\n",
      "         26773,   606,   286,   511,  6799,   454,    30,  1400,   438,  1640,\n",
      "            11, 31414,  1576,    11,   340,  2627,  4156,   326,   339,   373,\n",
      "         16245,   286,  9074,    13,   402,   271, 10899,   438,    69,   623,\n",
      "          1576,   407,   284,   766,   607, 41793,    13,   632,   373,   465,\n",
      "           898, 41793,   339,  3947,   284,   307,  1592,  2259,   739,   438,\n",
      "         14363,   898,  9408,   355,   281,  2134,   329,  5482,  4447,   290,\n",
      "           753,  1072,    13,   198,   198,     1,  3666, 13674,    11,  1201,\n",
      "           314,  1053,   442, 17758, 12036,   661,   836,   470,   910,   326,\n",
      "          3404,   546,   502,   438,  9930,   910,   340,   546, 12622, 41379,\n",
      "           293,   553,   373,   465,   691,  5402,    11,   355,   339,  8278,\n",
      "           422,   262,  3084,   290,   336,  8375],\n",
      "        [  438,   292,   339,   550,   587,   832,    11,   290,   287, 15275,\n",
      "           286,    11,   465,  5986,   438,   568, 22665,    11,   523, 23332,\n",
      "            11,   523,   595, 18052,    11,   326,   530,   890,   276,   284,\n",
      "          3960,   503,    25,   366,  3856, 44455,   351,   534, 24638,  2474,\n",
      "           355,  1752,   530,   550,   890,   276,   284,   910,    25,   366,\n",
      "          3856, 44455,   351,   534,   670,  2474,   198,   198,  1537,    11,\n",
      "           351,   262,  3960,   319,   616, 11914,    11,   616, 13669,  6989,\n",
      "           281, 10059,  2198,    13,   198,   198,     1,  1212,   318,   616,\n",
      "           898, 49451,   553,   339,   531,    11,  3756,   502,   656,   257,\n",
      "          3223,  8631,  2119,   379,   262,   886,   286,   262,   781,   273,\n",
      "           312,   410, 12523,    13,   632,   373,  6616,   290,  7586,   290,\n",
      "         11620,    88,    25,   645,   366, 34435,  8172,   645,   865,   291,\n",
      "            12,    64,    12,  1671,   330,    11,  4844,   286,   262,  1633,\n",
      "           286, 24380,   329, 20728,   287,   257,  4286, 10273,   438, 29370,\n",
      "           477,    11,   645,  1551,  1051,   286,  1683,  1719,   587,   973,\n",
      "           355,   257,  8034,    13,   198,   198,   464,  1109,  3181,  1363,\n",
      "           284,   502,   262,  4112,   957,  1483,   286,  3619,   338,  2270,\n",
      "           351,   465,  1468,  1204,    13,   198,   198,     1,  3987,   470,\n",
      "           345,  1683, 45553,   903,   351,  7521,   597,   517,  1701,   314,\n",
      "          1965,    11,   991,  2045,   546,   329,   257, 12854,   286,   884,\n",
      "          3842,    13,   198,   198,     1, 12295,   553,   339,   531, 11589,\n",
      "            13,   198,   198,     1,  5574,  1660,    12, 49903,   438,   273,\n",
      "          2123, 10813,  1701,   198,   198,  6653,  6563,  2951,  6348,  5391,\n",
      "            11,   290,   465, 25839,   279,  3021,   257,  1310,   739,   511,\n",
      "         22665,  4252, 10899,    13,   198,   198,     1, 12295,   892,   286,\n",
      "           340,    11,   616, 13674,  5891,   438]]) tensor([[ 1234,  8737,   656, 19133,   553,   373,   530,   286,   262,  7877,\n",
      "            72,  3150,   339,  8104,   866,  1973,   262, 37918,   411,   290,\n",
      "          8465,   286,   281, 33954,   271,  3973,  9899, 14678, 40556,    12,\n",
      "         11487,    11,   618,    11,   319,   257,  1568,  1110,    11,   314,\n",
      "           550,   757,  1057,   625,   422, 22489, 40089,    26,   290,  9074,\n",
      "            13,   402,   271, 10899,    11,   307,  3723,   319,   683,    11,\n",
      "          2087,   329,   616, 35957,    25,   366, 14295,   318,   523, 34813,\n",
      "           306,  8564,   284,   790,  1296,   286,  8737,   526,   198,   198,\n",
      "         43920,  3619,     0,   632,   550,  1464,   587,   465, 10030,   284,\n",
      "           423,  1466,   910,   884,  1243,   286,   683,    25,   262,  1109,\n",
      "           815,   307,   900,   866,   287,  1070,   268,  2288,    13,  1867,\n",
      "          7425,   502,   783,   373,   326,    11,   329,   262,   717,   640,\n",
      "            11,   339,   581,  4714,   262,  8216,    13,   314,   550,  1775,\n",
      "           683,    11,   523,  1690,    11,  1615,  3364,   739,  2092,   256,\n",
      "          7657,   438,  9776,   340,   262, 11644, 43778,  3465,   326, 26773,\n",
      "           606,   286,   511,  6799,   454,    30,  1400,   438,  1640,    11,\n",
      "         31414,  1576,    11,   340,  2627,  4156,   326,   339,   373, 16245,\n",
      "           286,  9074,    13,   402,   271, 10899,   438,    69,   623,  1576,\n",
      "           407,   284,   766,   607, 41793,    13,   632,   373,   465,   898,\n",
      "         41793,   339,  3947,   284,   307,  1592,  2259,   739,   438, 14363,\n",
      "           898,  9408,   355,   281,  2134,   329,  5482,  4447,   290,   753,\n",
      "          1072,    13,   198,   198,     1,  3666, 13674,    11,  1201,   314,\n",
      "          1053,   442, 17758, 12036,   661,   836,   470,   910,   326,  3404,\n",
      "           546,   502,   438,  9930,   910,   340,   546, 12622, 41379,   293,\n",
      "           553,   373,   465,   691,  5402,    11,   355,   339,  8278,   422,\n",
      "           262,  3084,   290,   336,  8375,   503],\n",
      "        [  292,   339,   550,   587,   832,    11,   290,   287, 15275,   286,\n",
      "            11,   465,  5986,   438,   568, 22665,    11,   523, 23332,    11,\n",
      "           523,   595, 18052,    11,   326,   530,   890,   276,   284,  3960,\n",
      "           503,    25,   366,  3856, 44455,   351,   534, 24638,  2474,   355,\n",
      "          1752,   530,   550,   890,   276,   284,   910,    25,   366,  3856,\n",
      "         44455,   351,   534,   670,  2474,   198,   198,  1537,    11,   351,\n",
      "           262,  3960,   319,   616, 11914,    11,   616, 13669,  6989,   281,\n",
      "         10059,  2198,    13,   198,   198,     1,  1212,   318,   616,   898,\n",
      "         49451,   553,   339,   531,    11,  3756,   502,   656,   257,  3223,\n",
      "          8631,  2119,   379,   262,   886,   286,   262,   781,   273,   312,\n",
      "           410, 12523,    13,   632,   373,  6616,   290,  7586,   290, 11620,\n",
      "            88,    25,   645,   366, 34435,  8172,   645,   865,   291,    12,\n",
      "            64,    12,  1671,   330,    11,  4844,   286,   262,  1633,   286,\n",
      "         24380,   329, 20728,   287,   257,  4286, 10273,   438, 29370,   477,\n",
      "            11,   645,  1551,  1051,   286,  1683,  1719,   587,   973,   355,\n",
      "           257,  8034,    13,   198,   198,   464,  1109,  3181,  1363,   284,\n",
      "           502,   262,  4112,   957,  1483,   286,  3619,   338,  2270,   351,\n",
      "           465,  1468,  1204,    13,   198,   198,     1,  3987,   470,   345,\n",
      "          1683, 45553,   903,   351,  7521,   597,   517,  1701,   314,  1965,\n",
      "            11,   991,  2045,   546,   329,   257, 12854,   286,   884,  3842,\n",
      "            13,   198,   198,     1, 12295,   553,   339,   531, 11589,    13,\n",
      "           198,   198,     1,  5574,  1660,    12, 49903,   438,   273,  2123,\n",
      "         10813,  1701,   198,   198,  6653,  6563,  2951,  6348,  5391,    11,\n",
      "           290,   465, 25839,   279,  3021,   257,  1310,   739,   511, 22665,\n",
      "          4252, 10899,    13,   198,   198,     1, 12295,   892,   286,   340,\n",
      "            11,   616, 13674,  5891,   438,  1092]])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "tensor([[   40,   367,  2885,  1464,  1807,  3619,   402,   271, 10899,  2138,\n",
      "           257,  7026, 15632,   438,  2016,   257,   922,  5891,  1576,   438,\n",
      "           568,   340,   373,   645,  1049,  5975,   284,   502,   284,  3285,\n",
      "           326,    11,   287,   262,  6001,   286,   465, 13476,    11,   339,\n",
      "           550,  5710,   465, 12036,    11,  6405,   257,  5527, 27075,    11,\n",
      "           290,  4920,  2241,   287,   257,  4489,    64,   319,   262, 34686,\n",
      "         41976,    13,   357, 10915,   314,  2138,  1807,   340,   561,   423,\n",
      "           587, 10598,   393, 28537,  2014,   198,   198,     1,   464,  6001,\n",
      "           286,   465, 13476,     1,   438,  5562,   373,   644,   262,  1466,\n",
      "          1444,   340,    13,   314,   460,  3285,  9074,    13, 46606,   536,\n",
      "          5469,   438, 14363,   938,  4842,  1650,   353,   438,  2934,   489,\n",
      "          3255,   465, 48422,   540,   450,    67,  3299,    13,   366,  5189,\n",
      "          1781,   340,   338,  1016,   284,  3758,   262,  1988,   286,   616,\n",
      "          4286,   705,  1014,   510,    26,   475,   314,   836,   470,   892,\n",
      "           286,   326,    11,  1770,    13,  8759,  2763,   438,  1169,  2994,\n",
      "           284,   943, 17034,   318,   477,   314,   892,   286,   526,   383,\n",
      "          1573,    11,   319,  9074,    13,   536,  5469,   338, 11914,    11,\n",
      "         33096,   663,  4808,  3808,    62,   355,   996,   484,   547, 12548,\n",
      "           287,   281, 13079,   410, 12523,   286, 22353,    13,   843,   340,\n",
      "           373,   407,   691,   262,  9074,    13,   536, 48819,   508, 25722,\n",
      "           276,    13, 11161,   407,   262, 40123, 18113,   544,  9325,   701,\n",
      "            11,   379,   262,   938,   402,  1617,   261, 12917,   905,    11,\n",
      "          5025,   502,   878,   402,   271, 10899,   338,   366, 31640,    12,\n",
      "            67, 20811,     1,   284,   910,    11,   351, 10953,   287,   607,\n",
      "          2951,    25,   366,  1135,  2236,   407,   804,  2402,   663,   588,\n",
      "           757, 13984,   198,   198,  5779, 28112],\n",
      "        [  673,  1908,   329,   345,  1701,   198,   198,     1,  5297,   438,\n",
      "         37121,  1035, 27339,   284,   262, 21296,    13,  1375,  2227,   683,\n",
      "         29178,  3474,   438,   392,   416,   502,  2474,   198,   198,  1544,\n",
      "         13818,   757,    11,   290,  9617,   736,   465,  1182,   284,   804,\n",
      "           510,   379,   262, 17548,   286,   262, 50085,    13,   366,  1858,\n",
      "           547,  1528,   618,   314,  3521,   470,   804,   379,   326,  1517,\n",
      "           438, 24089,    77,   470,  1986,   340,    13,   887,   314,  4137,\n",
      "          3589,   284,  1234,   340,   994,    26,   290,   783,   340,   338,\n",
      "         30703,   502,   438,    66,  1522,   502,    13,  1320,   338,   262,\n",
      "          1738,  1521,   314,   836,   470, 45553,   903,   597,   517,    11,\n",
      "           616, 13674,  8759,  2763,    26,   393,  2138,   520,  5493,  2241,\n",
      "           318,   262,  1738,   526,   198,   198,  1890,   262,   717,   640,\n",
      "           616, 21696, 20136,   546,   616, 15185,  2900,   656,   257,  2726,\n",
      "          6227,   284,  1833,   683,  1365,    13,   198,   198,     1,    40,\n",
      "          4601,   345,  1549,  1560,   502,   703,   340,  3022,   553,   314,\n",
      "           531,    13,   198,   198,  1544,  6204,  2045,   510,   379,   262,\n",
      "         17548,    11,   290,   665, 24297,  1022,   465,  9353,   257, 17779,\n",
      "           339,   550, 11564,   284,  1657,    13, 24975,   339,  2900,  3812,\n",
      "           502,    13,   198,   198,     1,    40,  1549,  2138,   588,   284,\n",
      "          1560,   345,   438, 13893,   314,  1053,  1464,  9885,   345,   286,\n",
      "          2376, 26927,   616,   670,   526,   198,   198,    40,   925,   257,\n",
      "          1207,  8344,   803, 18342,    11,   543,   339,  2469,   265,  1572,\n",
      "           351,   257,   922,    12, 17047,  8167, 32545,    13,   198,   198,\n",
      "             1,  5812,    11,   314,  1422,   470,  1337,   257, 14787,   618,\n",
      "           314,  4762,   287,  3589,   438,   392,   783,   340,   338,   281,\n",
      "          2087,  9839,  1022,   514,  2474,   198]]) tensor([[  367,  2885,  1464,  1807,  3619,   402,   271, 10899,  2138,   257,\n",
      "          7026, 15632,   438,  2016,   257,   922,  5891,  1576,   438,   568,\n",
      "           340,   373,   645,  1049,  5975,   284,   502,   284,  3285,   326,\n",
      "            11,   287,   262,  6001,   286,   465, 13476,    11,   339,   550,\n",
      "          5710,   465, 12036,    11,  6405,   257,  5527, 27075,    11,   290,\n",
      "          4920,  2241,   287,   257,  4489,    64,   319,   262, 34686, 41976,\n",
      "            13,   357, 10915,   314,  2138,  1807,   340,   561,   423,   587,\n",
      "         10598,   393, 28537,  2014,   198,   198,     1,   464,  6001,   286,\n",
      "           465, 13476,     1,   438,  5562,   373,   644,   262,  1466,  1444,\n",
      "           340,    13,   314,   460,  3285,  9074,    13, 46606,   536,  5469,\n",
      "           438, 14363,   938,  4842,  1650,   353,   438,  2934,   489,  3255,\n",
      "           465, 48422,   540,   450,    67,  3299,    13,   366,  5189,  1781,\n",
      "           340,   338,  1016,   284,  3758,   262,  1988,   286,   616,  4286,\n",
      "           705,  1014,   510,    26,   475,   314,   836,   470,   892,   286,\n",
      "           326,    11,  1770,    13,  8759,  2763,   438,  1169,  2994,   284,\n",
      "           943, 17034,   318,   477,   314,   892,   286,   526,   383,  1573,\n",
      "            11,   319,  9074,    13,   536,  5469,   338, 11914,    11, 33096,\n",
      "           663,  4808,  3808,    62,   355,   996,   484,   547, 12548,   287,\n",
      "           281, 13079,   410, 12523,   286, 22353,    13,   843,   340,   373,\n",
      "           407,   691,   262,  9074,    13,   536, 48819,   508, 25722,   276,\n",
      "            13, 11161,   407,   262, 40123, 18113,   544,  9325,   701,    11,\n",
      "           379,   262,   938,   402,  1617,   261, 12917,   905,    11,  5025,\n",
      "           502,   878,   402,   271, 10899,   338,   366, 31640,    12,    67,\n",
      "         20811,     1,   284,   910,    11,   351, 10953,   287,   607,  2951,\n",
      "            25,   366,  1135,  2236,   407,   804,  2402,   663,   588,   757,\n",
      "         13984,   198,   198,  5779, 28112, 10197],\n",
      "        [ 1908,   329,   345,  1701,   198,   198,     1,  5297,   438, 37121,\n",
      "          1035, 27339,   284,   262, 21296,    13,  1375,  2227,   683, 29178,\n",
      "          3474,   438,   392,   416,   502,  2474,   198,   198,  1544, 13818,\n",
      "           757,    11,   290,  9617,   736,   465,  1182,   284,   804,   510,\n",
      "           379,   262, 17548,   286,   262, 50085,    13,   366,  1858,   547,\n",
      "          1528,   618,   314,  3521,   470,   804,   379,   326,  1517,   438,\n",
      "         24089,    77,   470,  1986,   340,    13,   887,   314,  4137,  3589,\n",
      "           284,  1234,   340,   994,    26,   290,   783,   340,   338, 30703,\n",
      "           502,   438,    66,  1522,   502,    13,  1320,   338,   262,  1738,\n",
      "          1521,   314,   836,   470, 45553,   903,   597,   517,    11,   616,\n",
      "         13674,  8759,  2763,    26,   393,  2138,   520,  5493,  2241,   318,\n",
      "           262,  1738,   526,   198,   198,  1890,   262,   717,   640,   616,\n",
      "         21696, 20136,   546,   616, 15185,  2900,   656,   257,  2726,  6227,\n",
      "           284,  1833,   683,  1365,    13,   198,   198,     1,    40,  4601,\n",
      "           345,  1549,  1560,   502,   703,   340,  3022,   553,   314,   531,\n",
      "            13,   198,   198,  1544,  6204,  2045,   510,   379,   262, 17548,\n",
      "            11,   290,   665, 24297,  1022,   465,  9353,   257, 17779,   339,\n",
      "           550, 11564,   284,  1657,    13, 24975,   339,  2900,  3812,   502,\n",
      "            13,   198,   198,     1,    40,  1549,  2138,   588,   284,  1560,\n",
      "           345,   438, 13893,   314,  1053,  1464,  9885,   345,   286,  2376,\n",
      "         26927,   616,   670,   526,   198,   198,    40,   925,   257,  1207,\n",
      "          8344,   803, 18342,    11,   543,   339,  2469,   265,  1572,   351,\n",
      "           257,   922,    12, 17047,  8167, 32545,    13,   198,   198,     1,\n",
      "          5812,    11,   314,  1422,   470,  1337,   257, 14787,   618,   314,\n",
      "          4762,   287,  3589,   438,   392,   783,   340,   338,   281,  2087,\n",
      "          9839,  1022,   514,  2474,   198,   198]])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "tensor([[41186, 39614,  1386,    11,   287,   262, 13203,  5482,  1044,   276,\n",
      "          5739,    13,   383,  5019, 19001,   286,   262,  5739,  1444,   510,\n",
      "           477,   402,   271, 10899,   338,  1613,     0,   198,   198, 27034,\n",
      "            13,   402,   271, 10899,  9859,   736,   262,  4324,    12,    66,\n",
      "          3325,  1299,    11,  3888,  7263,   257,  4808,    73,   446,   259,\n",
      "         13235,    62,  1336,   286, 11398, 35560,  1000,   292,    11,  7121,\n",
      "           281,  3211,    12, 16337,  1497,    11,   290,   531,    25,   366,\n",
      "          1532,   345,  1302,   994,   345,   460,   655,  6687,   284,   766,\n",
      "           340,    13,   314,   550,   340,   625,   262, 24818,   417,    12,\n",
      "         12239,    11,   475,   339,  3636,   470,  1309,   340,  2652,   526,\n",
      "           198,   198,  5297,   438,    40,   714,   655,  6687,   284,   766,\n",
      "           340,   438,  1169,   717, 18560,   286,  3619,   338,   314,   550,\n",
      "          1683,   550,   284, 14022,   616,  2951,   625,     0, 19672,   484,\n",
      "           550,   262,  1295,   286, 15393,   438, 16706,   262,  4318,  6103,\n",
      "           287,   257, 14005,  7872,   393,  4808, 13698, 10322,  6532,    62,\n",
      "          8263,    12,  3823,    11,   393,   257, 36364,  1396,   417,  4624,\n",
      "           523,   326,   340,  1718,   262,  1657,   832, 41160,   286,  1468,\n",
      "          9932,   316,   666,   966,    13,   383,   517, 12949,  1295,  2627,\n",
      "           262,  4286,  1365,    26,  1865,    11,   355,   616,  2951,  6348,\n",
      "         23840,   284,   262,  2063,    12,  2971,    11,   477,   262, 16704,\n",
      "         14482,  1625,   503,   438,   439,   262, 10818, 20597, 32192,   355,\n",
      "          2709,   330,   871,    11,   262, 15910,   286, 16153,   312,   328,\n",
      "          3780,   416,   543,    11,   351,   884,  2784,  9830,  5032,    11,\n",
      "           339,  5257,   284, 36583,  3241,   422,   262,  1103,  1597,   286,\n",
      "           262,  4286,   284,   617,  2495, 11331,  2768,   590,   286,  3703,\n",
      "            13,  9074,    13,   402,   271, 10899],\n",
      "        [ 1310,  1165,   881, 40642,   972,  6654,   832,   616,  5975,    11,\n",
      "           329,   339,  9373,   351,   257,  1207,  8344,   803,  6487,    25,\n",
      "           366,  5297,   438,  7091,   338,   281, 12659,  2829,  1122,    11,\n",
      "           345,   760,    11,  9074,    13,   520,  5493,    13,  2332,   691,\n",
      "          2126,   373,   284,   423,   683,  1760,   416,   257, 38378, 34537,\n",
      "           438,   993,    11,  3595,   520,  5493,     0,  1375,  1807,   340,\n",
      "           262,  1654,   301,   835,   286, 46431,   465, 27951,   438,  1659,\n",
      "         10833,   340,   319,   257,  1308, 27461,  1171,    13,   843,   379,\n",
      "           262,  2589,   314,   373,  4808,  1169,    62, 38378, 34537,   526,\n",
      "           198,   198,     1, 10910,    11,  3595,   520,  5493,   438,   292,\n",
      "           345,   910,    13,  8920,  4808,  5562,    62,   465,  2106,  1701,\n",
      "           198,   198,     1,  2504,   373,   465,  2106,    13,  1375,  4762,\n",
      "           287,   683,    11, 26996,   798,   287,   683,   438,   273,  1807,\n",
      "           673,   750,    13,   887,   673,  3521,   470,  6842,   407,   284,\n",
      "           423,   477,   262,  8263,    12,  9649,   351,   607,    13,  1375,\n",
      "          3521,   470,  6842,   262,  1109,   326,    11,   319,  1401,    77,\n",
      "          3929,  1528,    11,   530,   714,  1464,   651,  1474,  1576,   284,\n",
      "           766,   465,  5986,    13, 23676,  2415,     0,  1375,   338,   655,\n",
      "           257, 24225, 39136,   278,   329,   584, 21441,    13,   520,  5493,\n",
      "           318,   262,   691,  2187,   314,  1683,  2993,   526,   198,   198,\n",
      "             1,  1639,  1683,  2993,    30,   887,   345,   655,   531,   438,\n",
      "             1,   198,   198,    38,   271, 10899,   550,   257, 11040,  8212,\n",
      "           287,   465,  2951,    13,   198,   198,     1,  5812,    11,   314,\n",
      "          2993,   683,    11,   290,   339,  2993,   502,   438,  8807,   340,\n",
      "          3022,   706,   339,   373,  2636,   526,   198,   198,    40,  5710,\n",
      "           616,  3809, 43045,    13,   366,  2215]]) tensor([[39614,  1386,    11,   287,   262, 13203,  5482,  1044,   276,  5739,\n",
      "            13,   383,  5019, 19001,   286,   262,  5739,  1444,   510,   477,\n",
      "           402,   271, 10899,   338,  1613,     0,   198,   198, 27034,    13,\n",
      "           402,   271, 10899,  9859,   736,   262,  4324,    12,    66,  3325,\n",
      "          1299,    11,  3888,  7263,   257,  4808,    73,   446,   259, 13235,\n",
      "            62,  1336,   286, 11398, 35560,  1000,   292,    11,  7121,   281,\n",
      "          3211,    12, 16337,  1497,    11,   290,   531,    25,   366,  1532,\n",
      "           345,  1302,   994,   345,   460,   655,  6687,   284,   766,   340,\n",
      "            13,   314,   550,   340,   625,   262, 24818,   417,    12, 12239,\n",
      "            11,   475,   339,  3636,   470,  1309,   340,  2652,   526,   198,\n",
      "           198,  5297,   438,    40,   714,   655,  6687,   284,   766,   340,\n",
      "           438,  1169,   717, 18560,   286,  3619,   338,   314,   550,  1683,\n",
      "           550,   284, 14022,   616,  2951,   625,     0, 19672,   484,   550,\n",
      "           262,  1295,   286, 15393,   438, 16706,   262,  4318,  6103,   287,\n",
      "           257, 14005,  7872,   393,  4808, 13698, 10322,  6532,    62,  8263,\n",
      "            12,  3823,    11,   393,   257, 36364,  1396,   417,  4624,   523,\n",
      "           326,   340,  1718,   262,  1657,   832, 41160,   286,  1468,  9932,\n",
      "           316,   666,   966,    13,   383,   517, 12949,  1295,  2627,   262,\n",
      "          4286,  1365,    26,  1865,    11,   355,   616,  2951,  6348, 23840,\n",
      "           284,   262,  2063,    12,  2971,    11,   477,   262, 16704, 14482,\n",
      "          1625,   503,   438,   439,   262, 10818, 20597, 32192,   355,  2709,\n",
      "           330,   871,    11,   262, 15910,   286, 16153,   312,   328,  3780,\n",
      "           416,   543,    11,   351,   884,  2784,  9830,  5032,    11,   339,\n",
      "          5257,   284, 36583,  3241,   422,   262,  1103,  1597,   286,   262,\n",
      "          4286,   284,   617,  2495, 11331,  2768,   590,   286,  3703,    13,\n",
      "          9074,    13,   402,   271, 10899,    11],\n",
      "        [ 1165,   881, 40642,   972,  6654,   832,   616,  5975,    11,   329,\n",
      "           339,  9373,   351,   257,  1207,  8344,   803,  6487,    25,   366,\n",
      "          5297,   438,  7091,   338,   281, 12659,  2829,  1122,    11,   345,\n",
      "           760,    11,  9074,    13,   520,  5493,    13,  2332,   691,  2126,\n",
      "           373,   284,   423,   683,  1760,   416,   257, 38378, 34537,   438,\n",
      "           993,    11,  3595,   520,  5493,     0,  1375,  1807,   340,   262,\n",
      "          1654,   301,   835,   286, 46431,   465, 27951,   438,  1659, 10833,\n",
      "           340,   319,   257,  1308, 27461,  1171,    13,   843,   379,   262,\n",
      "          2589,   314,   373,  4808,  1169,    62, 38378, 34537,   526,   198,\n",
      "           198,     1, 10910,    11,  3595,   520,  5493,   438,   292,   345,\n",
      "           910,    13,  8920,  4808,  5562,    62,   465,  2106,  1701,   198,\n",
      "           198,     1,  2504,   373,   465,  2106,    13,  1375,  4762,   287,\n",
      "           683,    11, 26996,   798,   287,   683,   438,   273,  1807,   673,\n",
      "           750,    13,   887,   673,  3521,   470,  6842,   407,   284,   423,\n",
      "           477,   262,  8263,    12,  9649,   351,   607,    13,  1375,  3521,\n",
      "           470,  6842,   262,  1109,   326,    11,   319,  1401,    77,  3929,\n",
      "          1528,    11,   530,   714,  1464,   651,  1474,  1576,   284,   766,\n",
      "           465,  5986,    13, 23676,  2415,     0,  1375,   338,   655,   257,\n",
      "         24225, 39136,   278,   329,   584, 21441,    13,   520,  5493,   318,\n",
      "           262,   691,  2187,   314,  1683,  2993,   526,   198,   198,     1,\n",
      "          1639,  1683,  2993,    30,   887,   345,   655,   531,   438,     1,\n",
      "           198,   198,    38,   271, 10899,   550,   257, 11040,  8212,   287,\n",
      "           465,  2951,    13,   198,   198,     1,  5812,    11,   314,  2993,\n",
      "           683,    11,   290,   339,  2993,   502,   438,  8807,   340,  3022,\n",
      "           706,   339,   373,  2636,   526,   198,   198,    40,  5710,   616,\n",
      "          3809, 43045,    13,   366,  2215,   673]])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "tensor([[  314,  4752,   340,  6777,    13,   632,   373,   407,   326,   616,\n",
      "          2583,   408,   373,   366, 47914,  1298,   319,   326,   966,   314,\n",
      "           714,   423,  1813,  4544,  9325,   701,   262, 40830, 12719,  3874,\n",
      "            13,   632,   373,   655,   780,   673,   373,  4808,  1662,    62,\n",
      "          3499,   438,   361,   314,   743,   307, 41746, 12004,   262,  6473,\n",
      "           438,  5562,   314,  1043,   607,   523,    13,  1114,  3619,    11,\n",
      "           477,   465,  1204,    11,   550,   587, 11191,   416,  3499,  1466,\n",
      "            25,   484,   550, 26546,  1068,   465,  1242,    11,   340,   550,\n",
      "           587,   302,  1144,   287,   262,  3024,    12,  4803,   286,   511,\n",
      "           512,  1741,    13,   843,   340,   373,  4361,  5048,   425,   284,\n",
      "          3465,   644,  1245,   262,   366, 25124,  3101,  8137,   286, 16957,\n",
      "          1696,   414,     1,   357,    40,  9577,  4544,  9325,   701,     8,\n",
      "           373,  1719,   319,   683,    13,   198,   198,    40,   423,  4750,\n",
      "           326,  9074,    13,   402,   271, 10899,   373,  5527,    26,   290,\n",
      "           340,   373,  3393, 34953,   856,   326,   607,  5229,   373, 37895,\n",
      "           422,   428, 25179,   257, 19217,   475,  8904, 14676,    13,   632,\n",
      "           318,    11,   355,   257,  3896,    11,   262,   661,   508, 40987,\n",
      "          1637,   508,   651,   749,   503,   286,   340,    26,   290,  3619,\n",
      "           338, 19992, 31564,   286,   465,  3656,   338,  1263,  5236,  9343,\n",
      "           683,    11,   351,   281,  5585,   286,  2818,   922,    12, 49705,\n",
      "            11,   284, 21595,  1133,   340,   656,  5563,   286,  1242,   290,\n",
      "         13064,    13,  1675,   262,  6846,    11,   314,  1276,   751,    11,\n",
      "           339,  6150,  5365, 31655,    26,   475,   339,   373,  7067, 29396,\n",
      "         18443, 12271,   290, 45592,    12, 14792,  5986,   351,   257,  8839,\n",
      "           326,  7284, 35924,   262, 12306,   395,  4133,    13,   198,   198,\n",
      "             1, 26788,   338,   691, 12226,   318],\n",
      "        [  503,  4291,   262,  4252, 18250,  8812,   558,    13,   198,   198,\n",
      "            40, 27846,   706,   683,    11,  7425,   416,   465,   938,  1573,\n",
      "            13, 12622, 41379,   293,   373,    11,   287,  1109,    11,  5033,\n",
      "           262,   582,   286,   262,  2589,   438,   292,  3619,  2241,    11,\n",
      "           530,  1244,  1234,   340,    11,   550,   587,   262,   582,   286,\n",
      "           262,  1711,    13,   383,  7099,  6802,   373,   531,   284,   423,\n",
      "          7042,  2241,   379,   616,  1545,   338,  3625,    11,   290,   314,\n",
      "         14028,   611,   257,   256, 11912,   286, 35394,   739, 10724,   262,\n",
      "          6846,   338, 11428,   450,    67,  3299,    13,   887,   645,   438,\n",
      "          1640,   340,   373,   407, 10597,   706,   326,  1785,   326,   262,\n",
      "          4808, 13698, 10322,  6532,    62,  8263,    12,  9649,   550,  9258,\n",
      "           284,  3359,   511,   366,  8642,   521,   829,   526,   198,   198,\n",
      "            40,  2900,   284,  9074,    13,   402,   271, 10899,    11,   508,\n",
      "           550, 18459,  1068,   284,  1577,   257, 23844,   286,  7543,   284,\n",
      "           607,   599,  6321,   287,   262, 17423,    12,  3823,    13,   198,\n",
      "           198,     1,  5195,  4808, 10134,    62,   339,   442, 17758, 12036,\n",
      "          1701,   314,  1965, 25891,    13,   198,   198,  3347,  4376,   607,\n",
      "         26928,   351,   257,  9254,   286,   922,    12, 17047,  8167,  5975,\n",
      "            13,   198,   198,     1,  5812,    11,   339,  1595,   470,  4808,\n",
      "         14150,    62,   284,   783,    11,   345,   760,    26,   290,   314,\n",
      "           765,   683,   284,  2883,  2241,   553,   673,   531,  2407,  2391,\n",
      "            13,   198,   198,    40,  3114,   546,   262, 40894,  2330,    12,\n",
      "          6839, 11978,  2119,    11,   351,   663,  4808, 44769,  8270,    12,\n",
      "           332,   660,    62,   410,  1386, 20394,   262, 23755,   286,   262,\n",
      "         14005,  1801,  2093, 41160,    11,   290,   663, 45592,    12, 14792,\n",
      "          1613,  1424,   287, 19217, 24887, 13431]]) tensor([[ 4752,   340,  6777,    13,   632,   373,   407,   326,   616,  2583,\n",
      "           408,   373,   366, 47914,  1298,   319,   326,   966,   314,   714,\n",
      "           423,  1813,  4544,  9325,   701,   262, 40830, 12719,  3874,    13,\n",
      "           632,   373,   655,   780,   673,   373,  4808,  1662,    62,  3499,\n",
      "           438,   361,   314,   743,   307, 41746, 12004,   262,  6473,   438,\n",
      "          5562,   314,  1043,   607,   523,    13,  1114,  3619,    11,   477,\n",
      "           465,  1204,    11,   550,   587, 11191,   416,  3499,  1466,    25,\n",
      "           484,   550, 26546,  1068,   465,  1242,    11,   340,   550,   587,\n",
      "           302,  1144,   287,   262,  3024,    12,  4803,   286,   511,   512,\n",
      "          1741,    13,   843,   340,   373,  4361,  5048,   425,   284,  3465,\n",
      "           644,  1245,   262,   366, 25124,  3101,  8137,   286, 16957,  1696,\n",
      "           414,     1,   357,    40,  9577,  4544,  9325,   701,     8,   373,\n",
      "          1719,   319,   683,    13,   198,   198,    40,   423,  4750,   326,\n",
      "          9074,    13,   402,   271, 10899,   373,  5527,    26,   290,   340,\n",
      "           373,  3393, 34953,   856,   326,   607,  5229,   373, 37895,   422,\n",
      "           428, 25179,   257, 19217,   475,  8904, 14676,    13,   632,   318,\n",
      "            11,   355,   257,  3896,    11,   262,   661,   508, 40987,  1637,\n",
      "           508,   651,   749,   503,   286,   340,    26,   290,  3619,   338,\n",
      "         19992, 31564,   286,   465,  3656,   338,  1263,  5236,  9343,   683,\n",
      "            11,   351,   281,  5585,   286,  2818,   922,    12, 49705,    11,\n",
      "           284, 21595,  1133,   340,   656,  5563,   286,  1242,   290, 13064,\n",
      "            13,  1675,   262,  6846,    11,   314,  1276,   751,    11,   339,\n",
      "          6150,  5365, 31655,    26,   475,   339,   373,  7067, 29396, 18443,\n",
      "         12271,   290, 45592,    12, 14792,  5986,   351,   257,  8839,   326,\n",
      "          7284, 35924,   262, 12306,   395,  4133,    13,   198,   198,     1,\n",
      "         26788,   338,   691, 12226,   318,   284],\n",
      "        [ 4291,   262,  4252, 18250,  8812,   558,    13,   198,   198,    40,\n",
      "         27846,   706,   683,    11,  7425,   416,   465,   938,  1573,    13,\n",
      "         12622, 41379,   293,   373,    11,   287,  1109,    11,  5033,   262,\n",
      "           582,   286,   262,  2589,   438,   292,  3619,  2241,    11,   530,\n",
      "          1244,  1234,   340,    11,   550,   587,   262,   582,   286,   262,\n",
      "          1711,    13,   383,  7099,  6802,   373,   531,   284,   423,  7042,\n",
      "          2241,   379,   616,  1545,   338,  3625,    11,   290,   314, 14028,\n",
      "           611,   257,   256, 11912,   286, 35394,   739, 10724,   262,  6846,\n",
      "           338, 11428,   450,    67,  3299,    13,   887,   645,   438,  1640,\n",
      "           340,   373,   407, 10597,   706,   326,  1785,   326,   262,  4808,\n",
      "         13698, 10322,  6532,    62,  8263,    12,  9649,   550,  9258,   284,\n",
      "          3359,   511,   366,  8642,   521,   829,   526,   198,   198,    40,\n",
      "          2900,   284,  9074,    13,   402,   271, 10899,    11,   508,   550,\n",
      "         18459,  1068,   284,  1577,   257, 23844,   286,  7543,   284,   607,\n",
      "           599,  6321,   287,   262, 17423,    12,  3823,    13,   198,   198,\n",
      "             1,  5195,  4808, 10134,    62,   339,   442, 17758, 12036,  1701,\n",
      "           314,  1965, 25891,    13,   198,   198,  3347,  4376,   607, 26928,\n",
      "           351,   257,  9254,   286,   922,    12, 17047,  8167,  5975,    13,\n",
      "           198,   198,     1,  5812,    11,   339,  1595,   470,  4808, 14150,\n",
      "            62,   284,   783,    11,   345,   760,    26,   290,   314,   765,\n",
      "           683,   284,  2883,  2241,   553,   673,   531,  2407,  2391,    13,\n",
      "           198,   198,    40,  3114,   546,   262, 40894,  2330,    12,  6839,\n",
      "         11978,  2119,    11,   351,   663,  4808, 44769,  8270,    12,   332,\n",
      "           660,    62,   410,  1386, 20394,   262, 23755,   286,   262, 14005,\n",
      "          1801,  2093, 41160,    11,   290,   663, 45592,    12, 14792,  1613,\n",
      "          1424,   287, 19217, 24887, 13431,    13]])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x, y)\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b9b1a4-863d-456f-a8dd-c07fb5c024ed",
   "metadata": {},
   "source": [
    "- Another optional check that the token sizes are in the expected ballpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb860488-5453-41d7-9870-23b723f742a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eb860488-5453-41d7-9870-23b723f742a0",
    "outputId": "96b9451a-9557-4126-d1c8-51610a1995ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3085e8-665e-48eb-bb41-cdde61537e06",
   "metadata": {},
   "source": [
    "- Next, we implement a utility function to calculate the cross-entropy loss of a given batch\n",
    "- In addition, we implement a second utility function to compute the loss for a user-specified number of batches in a data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b9de31e-4096-47b3-976d-b6d2fdce04bc",
   "metadata": {
    "id": "7b9de31e-4096-47b3-976d-b6d2fdce04bc"
   },
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0691332-84d0-48b3-b462-a885ddeb4fca",
   "metadata": {},
   "source": [
    "- If you have a machine with a CUDA-supported GPU, the LLM will train on the GPU without making any changes to the code\n",
    "- Via the `device` setting, we ensure that the data is loaded onto the same device as the LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56f5b0c9-1065-4d67-98b9-010e42fc1e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987583372328016\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43875e95-190f-4b17-8f9a-35034ba649ec",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model-1.webp\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9339f8d-00cb-4206-af67-58c32bd72055",
   "metadata": {
    "id": "b9339f8d-00cb-4206-af67-58c32bd72055"
   },
   "source": [
    "## 5.2 Training an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652a4cf4-e98f-46d9-bdec-60e7ccb8d6bd",
   "metadata": {},
   "source": [
    "- In this section, we finally implement the code for training the LLM\n",
    "- We focus on a simple training function (if you are interested in augmenting this training function with more advanced techniques, such as learning rate warmup, cosine annealing, and gradient clipping, please refer to [Appendix D](../../appendix-D/01_main-chapter-code))\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/train-steps.webp\" width=300px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "Mtp4gY0ZO-qq",
   "metadata": {
    "id": "Mtp4gY0ZO-qq"
   },
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a301b333-b9d4-4eeb-a212-3a9874e3ac47",
   "metadata": {},
   "source": [
    "- Now, let's train the LLM using the training function defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3422000b-7aa2-485b-92df-99372cd22311",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3422000b-7aa2-485b-92df-99372cd22311",
    "outputId": "0e046603-908d-4093-8ae5-ef2f632639fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.783, Val loss 9.927\n",
      "Ep 1 (Step 000005): Train loss 7.985, Val loss 8.335\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train loss 6.753, Val loss 7.048\n",
      "Ep 2 (Step 000015): Train loss 6.114, Val loss 6.573\n",
      "Every effort moves you, and,, and, and,,,,, and, and,,,,,,,,,,,,,, and,,,, and,, and,,,,, and,,,,,,\n",
      "Ep 3 (Step 000020): Train loss 5.525, Val loss 6.490\n",
      "Ep 3 (Step 000025): Train loss 5.324, Val loss 6.387\n",
      "Every effort moves you, and to the picture.                      \"I, and the of the of the's the honour, and, and I had been, and I\n",
      "Ep 4 (Step 000030): Train loss 4.761, Val loss 6.360\n",
      "Ep 4 (Step 000035): Train loss 4.461, Val loss 6.258\n",
      "Every effort moves you of the to the picture--as of the picture--as I had been \" it was his \" I was the     \"I was his I had been the his pictures--and it the picture and I had been the picture of\n",
      "Ep 5 (Step 000040): Train loss 3.833, Val loss 6.196\n",
      "Every effort moves you know the \"Oh, and he was not the fact by his last word.         \"I was.      \"Oh, I felt a little a little the    \n",
      "Ep 6 (Step 000045): Train loss 3.352, Val loss 6.139\n",
      "Ep 6 (Step 000050): Train loss 2.861, Val loss 6.112\n",
      "Every effort moves you know; and my dear, and he was not the fact with a little of the house of the fact of the fact, and.                       \n",
      "Ep 7 (Step 000055): Train loss 2.347, Val loss 6.138\n",
      "Ep 7 (Step 000060): Train loss 2.084, Val loss 6.179\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I looked--as of the fact, and I felt him--his back his head to the donkey. \"Oh, and_--because he had always _\n",
      "Ep 8 (Step 000065): Train loss 1.521, Val loss 6.176\n",
      "Ep 8 (Step 000070): Train loss 1.272, Val loss 6.178\n",
      "Every effort moves you?\" \"I didn't bear the picture--I told me.  \"I looked up, and went on groping and Mrs. I was back the head to look up at the honour being _mine_--because he was when I\n",
      "Ep 9 (Step 000075): Train loss 1.000, Val loss 6.277\n",
      "Ep 9 (Step 000080): Train loss 0.718, Val loss 6.281\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 10 (Step 000085): Train loss 0.506, Val loss 6.325\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to the donkey again. I saw that, and down the room, when I\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0WSRu2i0iHJE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "0WSRu2i0iHJE",
    "outputId": "9d36c61b-517d-4f07-a7e8-4563aff78b11"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc83ded-5f80-4e1c-bf4d-ccb59999d995",
   "metadata": {},
   "source": [
    "- Looking at the results above, we can see that the model starts out generating incomprehensible strings of words, whereas towards the end, it's able to produce grammatically more or less correct sentences\n",
    "- However, based on the training and validation set losses, we can see that the model starts overfitting\n",
    "- If we were to check a few passages it writes towards the end, we would find that they are contained in the training set verbatim -- it simply memorizes the training data\n",
    "- Later, we will cover decoding strategies that can mitigate this memorization by a certain degree\n",
    "- Note that the overfitting here occurs because we have a very, very small training set, and we iterate over it so many times\n",
    "  - The LLM training here primarily serves educational purposes; we mainly want to see that the model can learn to produce coherent text\n",
    "  - Instead of spending weeks or months on training this model on vast amounts of expensive hardware, we load pretrained weights later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb380c42-b31c-4ee1-b8b9-244094537272",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model-2.webp\" width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de713235-1561-467f-bf63-bf11ade383f0",
   "metadata": {},
   "source": [
    "**If you are interested in augmenting this training function with more advanced techniques, such as learning rate warmup, cosine annealing, and gradient clipping, please refer to [Appendix D](../../appendix-D/01_main-chapter-code)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5cdf2f-09a5-4eb0-a20a-d7aac5c14c2c",
   "metadata": {},
   "source": [
    "**If you are interested in a larger training dataset and longer training run, see [../03_bonus_pretraining_on_gutenberg](../03_bonus_pretraining_on_gutenberg)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699f45fc-bf78-42f2-bd24-2355db41b28f",
   "metadata": {
    "id": "699f45fc-bf78-42f2-bd24-2355db41b28f"
   },
   "source": [
    "## 5.3 Decoding strategies to control randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be9086e-2c27-41da-97d0-49137d0ba3c7",
   "metadata": {},
   "source": [
    "- Inference is relatively cheap with a relatively small LLM as the GPT model we trained above, so there's no need to use a GPU for it in case you used a GPU for training it above\n",
    "- Using the `generate_text_simple` function (from the previous chapter) that we used earlier inside the simple training function, we can generate new text one word (or token) at a time\n",
    "- As explained in section 5.1.2, the next generated token is the token corresponding to the largest probability score among all tokens in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2734cee0-f6f9-42d5-b71c-fa7e0ef28b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25dbe31-bb7c-4893-b25b-47d0492d4aa4",
   "metadata": {},
   "source": [
    "- Even if we execute the `generate_text_simple` function above multiple times, the LLM will always generate the same outputs\n",
    "- We now introduce two concepts, so-called decoding strategies, to modify the `generate_text_simple`: *temperature scaling* and *top-k* sampling\n",
    "- These will allow the model to control the randomness and diversity of the generated text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb6f380-a798-4fd9-825c-17b7cd29a994",
   "metadata": {},
   "source": [
    "### 5.3.1 Temperature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f4f53c-0612-43d3-aa82-52447eac50fa",
   "metadata": {},
   "source": [
    "- Previously, we always sampled the token with the highest probability as the next token using `torch.argmax`\n",
    "- To add variety, we can sample the next token using The `torch.multinomial(probs, num_samples=1)`, sampling from a probability distribution\n",
    "- Here, each index's chance of being picked corresponds to its probability in the input tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7531bae-d5de-44c0-bc78-78fed077e22a",
   "metadata": {},
   "source": [
    "- Here's a little recap of generating the next token, assuming a very small vocabulary for illustration purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "01a5ce39-3dc8-4c35-96bc-6410a1e42412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Suppose input is \"every effort moves you\", and the LLM\n",
    "# returns the following logits for the next token:\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "\n",
    "# The next generated token is then as follows:\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6400572f-b3c8-49e2-95bc-433e55c5b3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b23b863e-252a-403c-b5b1-62bc0a42319f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 x closer\n",
      "2 x every\n",
      "0 x effort\n",
      "544 x forward\n",
      "2 x inches\n",
      "1 x moves\n",
      "0 x pizza\n",
      "376 x toward\n",
      "4 x you\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123) # Manual seed for reproducibility\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63d0a27-830b-42b5-9986-6d1a7de04dd9",
   "metadata": {},
   "source": [
    "- Instead of determining the most likely token via `torch.argmax`, we use `torch.multinomial(probas, num_samples=1)` to determine the most likely token by sampling from the softmax distribution\n",
    "- For illustration purposes, let's see what happens when we sample the next token 1,000 times using the original softmax probabilities:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e7d9cf-a26d-4d9a-8664-4af1efa73832",
   "metadata": {},
   "source": [
    "- We can control the distribution and selection process via a concept called temperature scaling\n",
    "- \"Temperature scaling\" is just a fancy word for dividing the logits by a number greater than 0\n",
    "- Temperatures greater than 1 will result in more uniformly distributed token probabilities after applying the softmax\n",
    "- Temperatures smaller than 1 will result in more confident (sharper or more peaky) distributions after applying the softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0759e4c8-5362-467c-bec6-b0a19d1ba43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "# Temperature values\n",
    "temperatures = [1, 0.1, 5]  # Original, higher confidence, and lower confidence\n",
    "\n",
    "# Calculate scaled probabilities\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2e66e613-4aca-4296-a984-ddd0d80c6578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAEiCAYAAAA21pHjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABM5klEQVR4nO3deVxU1f8/8Newg2wimyAKiiYUO0q4oUWCGmqkGWooIt8scYFwjUUgwDQR/YRiKu5rRlqaJvIRcc0dMxEDREhBcSVA1jm/P/xxP44DyH7v4Pv5eMzjw5y5d+Y185l8zz333HNEjDEGQgghhAiSHN8BCCGEEFI/KtSEEEKIgFGhJoQQQgSMCjUhhBAiYFSoCSGEEAGjQk0IIYQIGBVqQgghRMCoUBNCCCECpsB3gPYmFotx7949aGhoQCQS8R2HEELIG4gxhn///RdGRkaQk2v4mPmNK9T37t2DiYkJ3zEIIYQQ5Ofno1u3bg1u88YVag0NDQAvPhxNTU2e0xBCCHkTFRcXw8TEhKtJDXnjCnVtd7empiYVakIIIbxqzClYGkxGCCGECBivhTotLQ0eHh4wMjKCSCTC/v37X7tPamoq7O3toaysDHNzc2zevLnNcxJCCCF84bVQl5aWwsbGBvHx8Y3a/vbt2xg1ahSGDRuGq1evYu7cuZg+fTp+//33Nk5KCCGE8IPXc9QjRozAiBEjGr19QkICzMzMsGLFCgCAhYUFTp06hZUrV8LNza2tYhJC2plYLEZlZSXfMQhpNkVFRcjLy7fKc8nUYLKzZ8/C1dVVos3NzQ1z586td5+KigpUVFRw94uLi9sqHiGkFVRWVuL27dsQi8V8RyGkRbS1tWFoaNjiOTtkqlAXFhbCwMBAos3AwADFxcV4/vw5VFVVpfaJiYlBeHh4e0UkhLQAYwwFBQWQl5eHiYnJayeCIESIGGMoKyvDgwcPAABdu3Zt0fPJVKFujkWLFiEwMJC7X3vtGiFEeKqrq1FWVgYjIyOoqanxHYeQZqs9cHzw4AH09fVb1A0uU4Xa0NAQ9+/fl2i7f/8+NDU16zyaBgBlZWUoKyu3RzxCGm+JVgOPPWu/HAJTU1MDAFBSUuI5CSEtV/tjs6qqqkWFWqb6lZydnZGSkiLRlpycDGdnZ54SEULaAs3DTzqC1voe81qoS0pKcPXqVVy9ehXAi8uvrl69iry8PAAvuq29vb257WfMmIGcnBzMnz8fN2/exJo1a7B3714EBATwEZ8QQghpc7wW6osXL8LOzg52dnYAgMDAQNjZ2SE0NBQAUFBQwBVtADAzM8OhQ4eQnJwMGxsbrFixAhs2bKBLswghhHRYvJ6jHjp0KBhj9T5e16xjQ4cOxZUrV9owFSFEaEwXHmrX18tdOqrR276uezMsLAxLlixpYSJhMTU1xdy5cxu8NFboZs+ejdOnT+P69euwsLDgenaFSKYGkxFCiNAUFBRwf+/ZswehoaHIzMzk2tTV1fmI1WSMMdTU1EBBof3KQmVlJa8DB6dNm4Y//vgD165d4y1DY8jUYDJCCBEaQ0ND7qalpQWRSCTRtnv3blhYWEBFRQV9+/bFmjVruH1zc3MhEomwd+9eDB48GKqqqujXrx9u3bqFCxcuwNHREerq6hgxYgSKioq4/aZOnYqxY8ciPDwcenp60NTUxIwZMyRmcxOLxYiJiYGZmRlUVVVhY2ODffv2cY+npqZCJBLh8OHDcHBwgLKyMk6dOoXs7GyMGTMGBgYGUFdXR79+/XDs2DFuv6FDh+LOnTsICAiASCTiehSWLFkCW1tbic8mLi4OpqamUrmjoqJgZGSEt956C8CLZYc/+eQTaGtrQ0dHB2PGjEFubm5r/N9Tr9WrV2PmzJno2bNnm75Oa6BCTQghbWTHjh0IDQ1FVFQUMjIyEB0djZCQEGzZskViu7CwMAQHB+Py5ctQUFDAxIkTMX/+fKxatQonT55EVlYWN3anVkpKCjIyMpCamopdu3YhKSlJYnKnmJgYbN26FQkJCfjrr78QEBCAyZMn48SJExLPs3DhQixduhQZGRmwtrZGSUkJRo4ciZSUFFy5cgXu7u7w8PDgxgslJSWhW7duiIiIQEFBgUSPQmOkpKQgMzMTycnJOHjwIKqqquDm5gYNDQ2cPHkSp0+fhrq6Otzd3RucRlZdXb3B24wZM5qUS8io65sQQtpIWFgYVqxYAU9PTwAvBsTeuHED69atw5QpU7jtgoKCuEGxc+bMgZeXF1JSUjBw4EAAgK+vr9SYHSUlJSQmJkJNTQ1vv/02IiIiMG/ePERGRqKqqgrR0dE4duwYd/lqz549cerUKaxbtw4uLi7c80REROCDDz7g7uvo6MDGxoa7HxkZiZ9//hm//PIL/P39oaOjA3l5eWhoaMDQ0LDJn0mnTp2wYcMGrst7+/btEIvF2LBhA3d0vmnTJmhrayM1NRXDhw+v83led05ZU1OzydmEigo1IYS0gdLSUmRnZ8PX1xd+fn5ce3V1NbS0JCe8sba25v6unSbZyspKoq12OspaNjY2ErO3OTs7o6SkBPn5+SgpKUFZWZlEAQZenBOuvcqmlqOjo8T9kpISLFmyBIcOHUJBQQGqq6vx/PlziStwWsLKykrivHR6ejqysrKgoaEhsV15eTmys7PrfR5zc/NWySMLqFATQkgbKCkpAQCsX78eTk5OEo+9OkuVoqIi93ftUeWrbU1ZpKT2tQ8dOgRjY2OJx16dqbFTp04S94OCgpCcnIzvvvsO5ubmUFVVxbhx4167mpmcnJzUVTxVVVVS2736eiUlJXBwcMCOHTukttXT06v39V43SG/y5MlISEhocBtZQYWaEELagIGBAYyMjJCTk4NJkya1+vOnp6dLLEZ07tw5qKurw8TEBDo6OlBWVkZeXp5EN3djnD59GlOnTsVHH30E4EUhfXVgl5KSEjfday09PT0UFhaCMcb92GjMJU/29vbYs2cP9PX1m9RdTV3fhBBCWiw8PByzZ8+GlpYW3N3dUVFRgYsXL+LJkycSiwU1R2VlJXx9fREcHIzc3FyEhYXB398fcnJy0NDQQFBQEAICAiAWizFo0CA8e/YMp0+fhqampsT58Vf17t0bSUlJ8PDwgEgkQkhIiNTRvKmpKdLS0vDpp59CWVkZurq6GDp0KIqKirBs2TKMGzcOR44cweHDh19bMCdNmoTly5djzJgxiIiIQLdu3XDnzh0kJSVh/vz56NatW537tbTrOysrCyUlJSgsLMTz58+5wm9paSm4ueZp1DchhLSR6dOnY8OGDdi0aROsrKzg4uKCzZs3w8zMrMXP/f7776N3794YMmQIJkyYgNGjR0tMrBIZGYmQkBDExMTAwsIC7u7uOHTo0GtfOzY2Fp07d8aAAQPg4eEBNzc32NvbS2wTERGB3Nxc9OrVi+uetrCwwJo1axAfHw8bGxucP38eQUFBr30fampqSEtLQ/fu3eHp6QkLCwv4+vqivLy8TY+Kp0+fDjs7O6xbtw63bt3iZsm8d+9em71mc4lYQ1ODdUDFxcXQ0tLCs2fPOlTXCJExtHpWncrLy3H79m2YmZlBRUWF7ziCNXXqVDx9+hT79+/nOwppQEPf56bUIjqiJoQQQgSMCjUhhBAiYDSYjBBCZExdCxaRjouOqAkhhBABo0JNCCGECBgVakIIIUTAqFATQgghAkaFmhBCCBEwKtSEEEKIgFGhJoSQFhCJRA3eXp7Ws6MwNTVFXFwc3zFaJC8vD6NGjYKamhr09fUxb948VFdXN7hPVFQUBgwYADU1NWhra7dPUNB11IQQWdDQlKtt8nqNn8a1oKCA+3vPnj0IDQ1FZmYm1/a65RiFgjGGmpoaKCi0X1morKzkZQGMmpoajBo1CoaGhjhz5gwKCgrg7e0NRUVFREdH17tfZWUlxo8fD2dnZ2zcuLHd8tIRNSGEtIChoSF309LSgkgkkmjbvXs3LCwsoKKigr59+2LNmjXcvrm5uRCJRNi7dy8GDx4MVVVV9OvXD7du3cKFCxfg6OgIdXV1jBgxAkVFRdx+U6dOxdixYxEeHg49PT1oampixowZEmtGi8VixMTEwMzMDKqqqrCxscG+ffu4x1NTUyESiXD48GE4ODhAWVkZp06dQnZ2NsaMGQMDAwOoq6ujX79+OHbsGLff0KFDcefOHQQEBHC9BgCwZMkS2NraSnw2cXFxMDU1lcodFRUFIyMjvPXWWwCA/Px8fPLJJ9DW1oaOjg7GjBkjtbRmazp69Chu3LiB7du3w9bWFiNGjEBkZCTi4+MbXHc7PDwcAQEBsLKyarNsdaFCTQghbWTHjh0IDQ1FVFQUMjIyEB0djZCQEGzZskViu7CwMAQHB+Py5ctQUFDAxIkTMX/+fKxatQonT55EVlYWQkNDJfZJSUlBRkYGUlNTsWvXLiQlJSE8PJx7PCYmBlu3bkVCQgL++usvBAQEYPLkyThx4oTE8yxcuBBLly5FRkYGrK2tUVJSgpEjRyIlJQVXrlyBu7s7PDw8kJeXBwBISkpCt27dEBERgYKCAokehcZISUlBZmYmkpOTcfDgQVRVVcHNzQ0aGho4efIkTp8+DXV1dbi7uzdYNNXV1Ru8zZgxo959z549CysrKxgYGHBtbm5uKC4uxl9//dWk99MeqOubEELaSFhYGFasWAFPT08AgJmZGW7cuIF169ZJrAkdFBQENzc3AMCcOXPg5eWFlJQUDBw4EADg6+srNW2okpISEhMToaamhrfffhsRERGYN28eIiMjUVVVhejoaBw7dgzOzs4AgJ49e+LUqVNYt24dXFxcuOeJiIjABx98wN3X0dGBjY0Ndz8yMhI///wzfvnlF/j7+0NHRwfy8vLQ0NCAoaFhkz+TTp06YcOGDVyX9/bt2yEWi7Fhwwbu6HzTpk3Q1tZGamoqhg8fXufz1K4fXZ+GVqQqLCyUKNIAuPuFhYWNfSvthgo1IYS0gdLSUmRnZ8PX1xd+fn5ce3V1NbS0JM+5W1tbc3/XFoyXu1cNDAzw4MEDiX1sbGygpqbG3Xd2dkZJSQny8/NRUlKCsrIyiQIMvDjHamdnJ9Hm6Ogocb+kpARLlizBoUOHUFBQgOrqajx//pw7om4pKysrifPS6enpyMrKgoaGhsR25eXlyM7Orvd5zM3NWyWPLKBCTQghbaCkpAQAsH79ejg5OUk8Ji8vL3FfUVGR+7v2qPLVNrFY3OTXPnToEIyNjSUeU1ZWlrjfqVMniftBQUFITk7Gd999B3Nzc6iqqmLcuHENdkMDgJycHBhjEm1VVVVS2736eiUlJXBwcMCOHTukttXT06v39V43SG/y5MlISEio8zFDQ0OcP39eou3+/fvcY0JDhZoQQtqAgYEBjIyMkJOTg0mTJrX686enp+P58+dQVVUFAJw7dw7q6uowMTGBjo4OlJWVkZeXJ9HN3RinT5/G1KlT8dFHHwF4UUhfHdilpKSEmpoaiTY9PT0UFhaCMcb92Hhd9zQA2NvbY8+ePdDX12+wu/pVLen6dnZ2RlRUFB48eAB9fX0AQHJyMjQ1NWFpadnoDO2FCjUhhLSR8PBwzJ49G1paWnB3d0dFRQUuXryIJ0+eIDAwsEXPXVlZCV9fXwQHByM3NxdhYWHw9/eHnJwcNDQ0EBQUhICAAIjFYgwaNAjPnj3D6dOnoampKXF+/FW9e/dGUlISPDw8IBKJEBISInU0b2pqirS0NHz66adQVlaGrq4uhg4diqKiIixbtgzjxo3DkSNHcPjw4dcW30mTJmH58uUYM2YMIiIi0K1bN9y5cwdJSUmYP38+unXrVud+Len6Hj58OCwtLfHZZ59h2bJlKCwsRHBwMGbOnMn1OJw/fx7e3t5ISUnheiXy8vLw+PFj5OXloaamhvuxYG5u3qaX4fE+6js+Ph6mpqZQUVGBk5OTVHfEq+Li4vDWW29BVVUVJiYmCAgIQHl5eTulJYSQxps+fTo2bNiATZs2wcrKCi4uLti8eTPMzMxa/Nzvv/8+evfujSFDhmDChAkYPXq0xOQqkZGRCAkJQUxMDCwsLODu7o5Dhw699rVjY2PRuXNnDBgwAB4eHnBzc4O9vb3ENhEREcjNzUWvXr247mkLCwusWbMG8fHxsLGxwfnz5xEUFPTa96Gmpoa0tDR0794dnp6esLCwgK+vL8rLy5t0hN0U8vLyOHjwIOTl5eHs7IzJkyfD29sbERER3DZlZWXIzMyU6L4PDQ2FnZ0dwsLCUFJSAjs7O9jZ2eHixYttkrOWiL16UqEd7dmzB97e3khISICTkxPi4uLw448/IjMzk+uOeNnOnTsxbdo0JCYmYsCAAbh16xamTp2KTz/9FLGxsY16zeLiYmhpaeHZs2dt9iUg5LUamsCjCZNtdDTl5eW4ffs2zMzMoKKiwnccwZo6dSqePn2K/fv38x2FNKCh73NTahGvR9SxsbHw8/ODj48PLC0tkZCQADU1NSQmJta5/ZkzZzBw4EBMnDgRpqamGD58OLy8vF57FE4IIYTIKt4KdWVlJS5dugRXV9f/hZGTg6urK86ePVvnPgMGDMClS5e4wpyTk4PffvsNI0eObJfMhBBCSHvjbTDZw4cPUVNTU+dF5zdv3qxzn4kTJ+Lhw4cYNGgQGGOorq7GjBkzsHjx4npfp6KiAhUVFdz94uLi1nkDhBDCk1cnPyEdG++DyZoiNTUV0dHRWLNmDS5fvoykpCQcOnQIkZGR9e4TExMDLS0t7mZiYtKOiQkhhJCW4e2IWldXF/Ly8txF5rXu379f7wXnISEh+OyzzzB9+nQAL2a4KS0txf/93//h66+/hpyc9O+ORYsWSVwGUVxcTMWaEEKIzODtiFpJSQkODg5ISUnh2sRiMVJSUri5aV9VVlYmVYxrZ/ipb/C6srIyNDU1JW6EEEKIrOB1wpPAwEBMmTIFjo6O6N+/P+Li4lBaWgofHx8AgLe3N4yNjRETEwMA8PDwQGxsLOzs7ODk5ISsrCyEhITAw8NDako+QgghpCPgtVBPmDABRUVFCA0NRWFhIWxtbXHkyBFugFleXp7EEXRwcDBEIhGCg4Nx9+5d6OnpwcPDA1FRUXy9BUIIIaRN8TrhCR9owhMiCDThSZ1owhPSkXSICU8IIYQQ0jAq1IQQ0gIikajB28vzb3cUpqamiIuL4ztGi9T1/9Xu3bv5jlUnWj2LECJ4Vlus2vX1/pzyZ6O3LSgo4P7es2cPQkNDkZmZybW15apKrYkxhpqaGigotF9ZqKyshJKSUru93qs2bdoEd3d37r62tjZvWRpCR9SEENIChoaG3E1LSwsikUiibffu3bCwsICKigr69u2LNWvWcPvm5uZCJBJh7969GDx4MFRVVdGvXz/cunULFy5cgKOjI9TV1TFixAgUFRVx+02dOhVjx45FeHg49PT0oKmpiRkzZqCyspLbRiwWIyYmBmZmZlBVVYWNjQ327dvHPZ6amgqRSITDhw/DwcEBysrKOHXqFLKzszFmzBgYGBhAXV0d/fr1w7Fjx7j9hg4dijt37iAgIIA7EgWAJUuWwNbWVuKziYuLg6mpqVTuqKgoGBkZ4a233gIA5Ofn45NPPoG2tjZ0dHQwZswYqTWw24K2trbE/1dCHRdBhZoQQtrIjh07EBoaiqioKGRkZCA6OhohISHYsmWLxHZhYWEIDg7G5cuXoaCggIkTJ2L+/PlYtWoVTp48iaysLISGhkrsk5KSgoyMDKSmpmLXrl1ISkpCeHg493hMTAy2bt2KhIQE/PXXXwgICMDkyZNx4sQJiedZuHAhli5dioyMDFhbW6OkpAQjR45ESkoKrly5And3d3h4eCAvLw8AkJSUhG7duiEiIgIFBQUSPQqNkZKSgszMTCQnJ+PgwYOoqqqCm5sbNDQ0cPLkSZw+fRrq6upwd3eX+OHxKnV19QZvM2bMeG2WmTNnQldXF/3790diYmK983Hwjbq+CSGkjYSFhWHFihXw9PQEAJiZmeHGjRtYt24dpkyZwm0XFBQENzc3AMCcOXPg5eWFlJQUDBw4EADg6+srNb+3kpISEhMToaamhrfffhsRERGYN28eIiMjUVVVhejoaBw7doybQKpnz544deoU1q1bBxcXF+55IiIi8MEHH3D3dXR0YGNjw92PjIzEzz//jF9++QX+/v7Q0dGBvLw8NDQ06p1FsiGdOnXChg0buC7v7du3QywWY8OGDdzR+aZNm6CtrY3U1FQMHz68zue5evVqg6/zupHUEREReO+996CmpoajR4/iyy+/RElJCWbPnt3k99TWqFATQkgbKC0tRXZ2Nnx9feHn58e1V1dXQ0tL8vI8a2tr7u/aeSSsrKwk2h48eCCxj42NDdTU1Lj7zs7OKCkpQX5+PkpKSlBWViZRgIEX54Tt7Owk2hwdHSXul5SUYMmSJTh06BAKCgpQXV2N58+fc0fULWVlZSVxXjo9PR1ZWVnQ0NCQ2K68vBzZ2dn1Po+5uXmLcoSEhHB/29nZobS0FMuXL6dCTQghb4qSkhIAwPr16+Hk5CTx2KszKSoqKnJ/1x5VvtomFoub/NqHDh2CsbGxxGPKysoS9zt16iRxPygoCMnJyfjuu+9gbm4OVVVVjBs3rsFuaODFMsWvdh1XVVVJbffq65WUlMDBwQE7duyQ2lZPT6/e13vdIL3JkycjISGhwW1e5uTkhMjISFRUVEh9RnyjQk0IIW3AwMAARkZGyMnJwaRJk1r9+dPT0/H8+XOoqqoCAM6dOwd1dXWYmJhAR0cHysrKyMvLk+jmbozTp09j6tSp+OijjwC8KKSvDuxSUlJCTU2NRJuenh4KCwvBGON+bLyuexoA7O3tsWfPHujr6zdpEqqWdn3X9XydO3cWXJEGqFATQkibCQ8Px+zZs6GlpQV3d3dUVFTg4sWLePLkicSqfs1RWVkJX19fBAcHIzc3F2FhYfD394ecnBw0NDQQFBSEgIAAiMViDBo0CM+ePcPp06ehqakpcX78Vb1790ZSUhI8PDwgEokQEhIidTRvamqKtLQ0fPrpp1BWVoauri6GDh2KoqIiLFu2DOPGjcORI0dw+PDh1xbMSZMmYfny5RgzZgwiIiLQrVs33LlzB0lJSZg/fz66detW534t6fr+9ddfcf/+fbz77rtQUVFBcnIyoqOjERQU1OznbEs06psQQtrI9OnTsWHDBmzatAlWVlZwcXHB5s2bYWZm1uLnfv/999G7d28MGTIEEyZMwOjRoyUmV4mMjERISAhiYmJgYWEBd3d3HDp06LWvHRsbi86dO2PAgAHw8PCAm5sb7O3tJbaJiIhAbm4uevXqxXVPW1hYYM2aNYiPj4eNjQ3Onz/fqMKnpqaGtLQ0dO/eHZ6enrCwsICvry/Ky8vbbJpnRUVFxMfHw9nZGba2tli3bh1iY2MRFhbWJq/XUjTXNyF8oLm+60RzfTfO1KlT8fTpU+zfv5/vKKQBNNc3IYQQ8gagQk0IIYQIGA0mI4QQGfPq5CekY2vWEfXx48dbOwchhBBC6tCsQu3u7o5evXrhm2++QX5+fmtnIoQQQsj/16xCfffuXfj7+2Pfvn3o2bMn3NzcsHfv3tfOXEMIIY3xhl2MQjqo1voeN6tQ6+rqIiAgAFevXsUff/yBPn364Msvv4SRkRFmz56N9PT0VglHCHmz1E6tST/6SUdQVlYGQHI62OZo8WAye3t7GBoaokuXLli6dCkSExOxZs0aODs7IyEhAW+//XZLX4IQ8oZQUFCAmpoaioqKoKioCDk5ujCFyB7GGMrKyvDgwQNoa2tLze3eVM0u1FVVVThw4AASExORnJwMR0dHfP/99/Dy8kJRURGCg4Mxfvx43Lhxo0UBCSFvDpFIhK5du+L27du4c+cO33EIaRFtbe1mLQX6qmYV6lmzZmHXrl1gjOGzzz7DsmXL8M4773CPd+rUCd999x2MjIxaHJAQ8mZRUlJC7969qfubyDRFRcUWH0nXalahvnHjBv7zn//A09Oz3pVGdHV16TIuQkizyMnJ0RSihPx/zToBFBYWhvHjx0sV6erqaqSlpQF4ca6pqcurEUIIIURSswr1sGHD8PjxY6n2Z8+eYdiwYS0ORQghhJAXmlWoX14Y/GWPHj1Cp06dWhyKEEIIIS806Ry1p6cngBcjM6dOnSrR9V1TU4Nr165hwIABrZuQEEIIeYM1qVBrab1YQ5cxBg0NDaiqqnKPKSkp4d1334Wfn1/rJiSEEELeYE0q1Js2bQIAmJqaIigoiLq5CSGEkDbW7FHfrVWk4+PjYWpqChUVFTg5OeH8+fMNbv/06VPMnDkTXbt2hbKyMvr06YPffvutVbIQQgghQtPoI2p7e3ukpKSgc+fOsLOzq3MwWa3Lly836jn37NmDwMBAJCQkwMnJCXFxcXBzc0NmZib09fWltq+srMQHH3wAfX197Nu3D8bGxrhz5w60tbUb+zYIIYQQmdLoQj1mzBhu8NjYsWNb5cVjY2Ph5+cHHx8fAEBCQgIOHTqExMRELFy4UGr7xMREPH78GGfOnOEmOTc1NW2VLIQQQogQiRhP68lVVlZCTU0N+/btkyj8U6ZMwdOnT3HgwAGpfUaOHAkdHR2oqanhwIED0NPTw8SJE7FgwYJ6p2qrqKhARUUFd7+4uBgmJiZ49uwZNDU1W/19EdIoS7QaeOxZ++UghPCiuLgYWlpajapFvC1N8/DhQ9TU1MDAwECi3cDAAIWFhXXuk5OTg3379qGmpga//fYbQkJCsGLFCnzzzTf1vk5MTAy0tLS4m4mJSau+D0IIIaQtNbrru3Pnzg2el35ZXbOWtQaxWAx9fX388MMPkJeXh4ODA+7evYvly5cjLCyszn0WLVqEwMBA7n7tETUhhBAiCxpdqOPi4lr1hXV1dSEvL4/79+9LtN+/f7/eZcG6du0qtSKJhYUFCgsLUVlZCSUlJal9lJWV6104hBBCCBG6RhfqKVOmtOoLKykpwcHBASkpKdw5arFYjJSUFPj7+9e5z8CBA7Fz506IxWJuQflbt26ha9eudRZpQgghRNY1+hx1cXGxxN8N3RorMDAQ69evx5YtW5CRkYEvvvgCpaWl3Chwb29vLFq0iNv+iy++wOPHjzFnzhzcunULhw4dQnR0NGbOnNno1ySEEEJkSZPOURcUFEBfXx/a2tp1nq+uXayjpqamUc85YcIEFBUVITQ0FIWFhbC1tcWRI0e4AWZ5eXnckTMAmJiY4Pfff0dAQACsra1hbGyMOXPmYMGCBY19G4QQQohMafTlWSdOnMDAgQOhoKCAEydONLitkNehbsqQeEJawnThoXofy1WZWP+OdHkWIR1eU2pRo4+oXy6+Qi7EhBBCSEfSpEU5XvbkyRNs3LgRGRkZAABLS0v4+PhAR0en1cIRQgghb7pmTXiSlpYGU1NTrF69Gk+ePMGTJ0+wevVqmJmZIS0trbUzEkIIIW+sZh1Rz5w5ExMmTMDatWu5a5pramrw5ZdfYubMmfjzzz9bNSQhhBDypmrWEXVWVha++uoriYlH5OXlERgYiKysrFYLRwghhLzpmlWo7e3tuXPTL8vIyICNjU2LQxFCCCHkhUZ3fV+7do37e/bs2ZgzZw6ysrLw7rvvAgDOnTuH+Ph4LF26tPVTEkIIIW+oRl9HLScnB5FIhNdt3pQJT/hA11GT9kLXURNC6tMm11Hfvn27xcEIIYQQ0jSNLtQ9evRoyxyEEEIIqUOzJzwBgBs3biAvLw+VlZUS7aNHj25RKEIIIYS80KxCnZOTg48++gh//vmnxHnr2oU6hHyOmhBCCJElzbo8a86cOTAzM8ODBw+gpqaGv/76C2lpaXB0dERqamorRySEEELeXM06oj579iz++9//QldXF3JycpCTk8OgQYMQExOD2bNn48qVK62dkxBCCHkjNeuIuqamBhoaGgAAXV1d3Lt3D8CLAWeZmZmtl44QQgh5wzXriPqdd95Beno6zMzM4OTkhGXLlkFJSQk//PADevbs2doZCSGEkDdWswp1cHAwSktLAQARERH48MMPMXjwYHTp0gV79uxp1YCEEELIm6xZhdrNzY3729zcHDdv3sTjx4/RuXNnbuQ3IYQQQlquRddRA0B+fj4AwMTEpMVhCCGEECKpWYPJqqurERISAi0tLZiamsLU1BRaWloIDg5GVVVVa2ckhBBC3ljNOqKeNWsWkpKSsGzZMjg7OwN4ccnWkiVL8OjRI6xdu7ZVQxJCCCFvqmYV6p07d2L37t0YMWIE12ZtbQ0TExN4eXlRoSaEEEJaSbO6vpWVlWFqairVbmZmBiUlpZZmIoQQQsj/16xC7e/vj8jISFRUVHBtFRUViIqKgr+/f6uFI4QQQt50je769vT0lLh/7NgxdOvWDTY2NgCA9PR0VFZW4v3332/dhIQQQsgbrNGFWktLS+L+xx9/LHGfLs8ihBBCWl+jC/WmTZvaMgchhBBC6tCiCU+Kioq4RTjeeust6OnptUooQgghhLzQrMFkpaWlmDZtGrp27YohQ4ZgyJAhMDIygq+vL8rKylo7IyGEEPLGalahDgwMxIkTJ/Drr7/i6dOnePr0KQ4cOIATJ07gq6++avLzxcfHw9TUFCoqKnBycsL58+cbtd/u3bshEokwduzYJr8mIYQQIguaVah/+uknbNy4ESNGjICmpiY0NTUxcuRIrF+/Hvv27WvSc+3ZsweBgYEICwvD5cuXYWNjAzc3Nzx48KDB/XJzcxEUFITBgwc35y0QQgghMqFZhbqsrAwGBgZS7fr6+k3u+o6NjYWfnx98fHxgaWmJhIQEqKmpITExsd59ampqMGnSJISHh9P614QQQjq0ZhVqZ2dnhIWFoby8nGt7/vw5wsPDubm/G6OyshKXLl2Cq6vr/wLJycHV1RVnz56td7+IiAjo6+vD19f3ta9RUVGB4uJiiRshhBAiK5o16jsuLg7u7u5SE56oqKjg999/b/TzPHz4EDU1NVJH5wYGBrh582ad+5w6dQobN27E1atXG/UaMTExCA8Pb3QmQgghREiaVaitrKzw999/Y8eOHVxB9fLywqRJk6CqqtqqAV/277//4rPPPsP69euhq6vbqH0WLVqEwMBA7n5xcTFNzkIIIURmNLlQV1VVoW/fvjh48CD8/Pxa9OK6urqQl5fH/fv3Jdrv378PQ0NDqe2zs7ORm5sLDw8Prk0sFgMAFBQUkJmZiV69eknso6ysDGVl5RblJIQQQvjS5HPUioqKEuemW0JJSQkODg5ISUnh2sRiMVJSUuo81923b1/8+eefuHr1KncbPXo0hg0bhqtXr9KRMiGEkA6nWV3fM2fOxLfffosNGzZAQaFFk5shMDAQU6ZMgaOjI/r374+4uDiUlpbCx8cHAODt7Q1jY2PExMRARUUF77zzjsT+2traACDVTgghhHQEzaqyFy5cQEpKCo4ePQorKyt06tRJ4vGkpKRGP9eECRNQVFSE0NBQFBYWwtbWFkeOHOEGmOXl5UFOrlmD0wkhhBCZ16xCra2tLbV6Vkv4+/vXu451ampqg/tu3ry51XIQQgghQtOkQi0Wi7F8+XLcunULlZWVeO+997BkyZI2HelNCCGEvMma1KccFRWFxYsXQ11dHcbGxli9ejVmzpzZVtkIIYSQN16Tjqi3bt2KNWvW4PPPPwcAHDt2DKNGjcKGDRvoPDIhhHRwpgsP1dmeu3RUOyd5szSpuubl5WHkyJHcfVdXV4hEIty7d6/VgxFCCCGkiYW6uroaKioqEm2Kioqoqqpq1VCEEEIIeaFJXd+MMUydOlVipq/y8nLMmDFD4hKtplyeRQghhJD6NalQT5kyRapt8uTJrRaGEEIIIZKaVKg3bdrUVjkIIYQQUgcaqk0IIYQIGBVqQgghRMCoUBNCCCECRoWaEEIIETAq1IQQQoiAUaEmhBBCBIwKNSGEECJgVKgJIYQQAaNCTQghhAgYFWpCCCFEwKhQE0IIIQJGhZoQQggRMCrUhBBCiIBRoSaEEEIEjAo1IYQQImBUqAkhhBABo0JNCCGECJgC3wEIIZKstljV+9ifU/5sxySEECGgI2pCCCFEwKhQE0IIIQImiEIdHx8PU1NTqKiowMnJCefPn6932/Xr12Pw4MHo3LkzOnfuDFdX1wa3J4QQQmQZ7+eo9+zZg8DAQCQkJMDJyQlxcXFwc3NDZmYm9PX1pbZPTU2Fl5cXBgwYABUVFXz77bcYPnw4/vrrLxgbG/PwDgghhNSHxly0HO9H1LGxsfDz84OPjw8sLS2RkJAANTU1JCYm1rn9jh078OWXX8LW1hZ9+/bFhg0bIBaLkZKS0s7JCSGEkLbHa6GurKzEpUuX4OrqyrXJycnB1dUVZ8+ebdRzlJWVoaqqCjo6Om0VkxBCCOENr13fDx8+RE1NDQwMDCTaDQwMcPPmzUY9x4IFC2BkZCRR7F9WUVGBiooK7n5xcXHzAxNCCCHtjPeu75ZYunQpdu/ejZ9//hkqKip1bhMTEwMtLS3uZmJi0s4pCSGEkObjtVDr6upCXl4e9+/fl2i/f/8+DA0NG9z3u+++w9KlS3H06FFYW1vXu92iRYvw7Nkz7pafn98q2QkhhJD2wGuhVlJSgoODg8RAsNqBYc7OzvXut2zZMkRGRuLIkSNwdHRs8DWUlZWhqakpcSOEEEJkBe+XZwUGBmLKlClwdHRE//79ERcXh9LSUvj4+AAAvL29YWxsjJiYGADAt99+i9DQUOzcuROmpqYoLCwEAKirq0NdXZ2390EIIYS0Bd4L9YQJE1BUVITQ0FAUFhbC1tYWR44c4QaY5eXlQU7ufwf+a9euRWVlJcaNGyfxPGFhYViyZEl7RieEEELaHO+FGgD8/f3h7+9f52OpqakS93Nzc9s+ECGEECIQMj3qmxBCCOnoqFATQgghAkaFmhBCCBEwQZyjfhPRRPWEEEIag46oCSGEEAGjQk0IIYQIGBVqQgghRMCoUBNCCCECRoWaEEIIETAq1IQQQoiAUaEmhBBCBIwKNSGEECJgVKgJIYQQAaNCTQghhAgYFWpCCCFEwKhQE0IIIQJGi3IQQlqMFpkhHYnQvs90RE0IIYQIGBVqQgghRMCo65s0mtC6gwgh5E1AR9SEEEKIgFGhJoQQQgSMur5byHThoXofy106qh2TEEII6YjoiJoQQggRMCrUhBBCiIBR1zfp0GikOqmPLH43ZDEzaTk6oiaEEEIEjAo1IYQQImBUqAkhhBABE0Shjo+Ph6mpKVRUVODk5ITz5883uP2PP/6Ivn37QkVFBVZWVvjtt9/aKSkhhBDSvngv1Hv27EFgYCDCwsJw+fJl2NjYwM3NDQ8ePKhz+zNnzsDLywu+vr64cuUKxo4di7Fjx+L69evtnJwQQghpe7wX6tjYWPj5+cHHxweWlpZISEiAmpoaEhMT69x+1apVcHd3x7x582BhYYHIyEjY29vj+++/b+fkhBBCSNvj9fKsyspKXLp0CYsWLeLa5OTk4OrqirNnz9a5z9mzZxEYGCjR5ubmhv3797dlVEIIIfVZolX/Y2bd2y9HB8VroX748CFqampgYGAg0W5gYICbN2/WuU9hYWGd2xcWFta5fUVFBSoqKrj7z549AwAUFxe3JDpHXFFW72MNvUbN85pm7dca3gn7vd7Hroe71fsYn5mbi8/MDX43RKzex/j+nOv7ftB3g398Z67vO03f56arfR7G6v/sOIxHd+/eZQDYmTNnJNrnzZvH+vfvX+c+ioqKbOfOnRJt8fHxTF9fv87tw8LCGAC60Y1udKMb3QR3y8/Pf22t5PWIWldXF/Ly8rh//75E+/3792FoaFjnPoaGhk3aftGiRRJd5WKxGI8fP0aXLl0gEola+A4kFRcXw8TEBPn5+dDU1GzV524rlLl9UOb2QZnbB2VuOcYY/v33XxgZGb12W14LtZKSEhwcHJCSkoKxY8cCeFFIU1JS4O/vX+c+zs7OSElJwdy5c7m25ORkODs717m9srIylJWVJdq0tbVbI369NDU1BfFFaArK3D4oc/ugzO2DMreMlpZWo7bjfa7vwMBATJkyBY6Ojujfvz/i4uJQWloKHx8fAIC3tzeMjY0RExMDAJgzZw5cXFywYsUKjBo1Crt378bFixfxww8/8Pk2CCGEkDbBe6GeMGECioqKEBoaisLCQtja2uLIkSPcgLG8vDzIyf3vKrIBAwZg586dCA4OxuLFi9G7d2/s378f77zzDl9vgRBCCGkzvBdqAPD396+3qzs1NVWqbfz48Rg/fnwbp2o6ZWVlhIWFSXW1Cxllbh+UuX1Q5vZBmduXiLHGjA0nhBBCCB94n5mMEEIIIfWjQk0IIYQIGBVqQgghRMCoUBNCCCECRoW6maqrq7F161apWdIIIYSQ1kSjvltATU0NGRkZ6NGjB99RGm3KlCnw9fXFkCFD+I7SJD179sSFCxfQpUsXifanT5/C3t4eOTk5PCX7n19++aXR244ePboNk7zZampq8Oeff6JHjx7o3Lkz33FkVlMWnxDKTF+vSktLa/BxWfl3UBDXUcuq/v374+rVqzJVqJ89ewZXV1f06NEDPj4+mDJlCoyNjfmO9Vq5ubmoqZFe0aaiogJ3797lIZG02mlwa4lEIomVcV6eW76u9yIEW7Zsga6uLkaNGgUAmD9/Pn744QdYWlpi165dgvyuz507F1ZWVvD19UVNTQ1cXFxw5swZqKmp4eDBgxg6dCjfEWWStrZ2o9dDEOr3ua7/72Xhv8NXUaFugS+//BKBgYHIz8+Hg4MDOnXqJPG4tbU1T8nqt3//fhQVFWHbtm3YsmULwsLC4OrqCl9fX4wZMwaKiop8R5Tw8lHq77//LjE3bk1NDVJSUmBqaspDMmlisZj7+9ixY1iwYAGio6O5eejPnj2L4OBgREdH8xXxtaKjo7F27VoAL/LGx8dj5cqVOHjwIAICApCUlMRzQmn79u3D5MmTAQC//vorbt++jZs3b2Lbtm34+uuvcfr0aZ4T1m3fvn3Yu3cv8vLyUFlZKfHY5cuXeUr1P8ePH+f+zs3NxcKFCzF16lSJ7/OWLVu46Z2F6MmTJxL3q6qqcOXKFYSEhCAqKoqnVM3w2vW1SL1EIpHUTU5OjvtfWXDp0iXm7+/PVFRUmK6uLps7dy67desW37E4dX3GtTclJSXWp08f9uuvv/IdU8rbb7/NTp48KdWelpbG+vbty0OixlFVVWV37txhjDE2f/589tlnnzHGGLt+/TrT1dXlM1q9lJWVuaUC/fz82Jw5cxhjjOXk5DANDQ0ek9Vv1apVTF1dnfn7+zMlJSX2+eefM1dXV6alpcUWL17Mdzwp7733ntTywowxtmPHDubi4tL+gVooNTWV2dvb8x2j0WgwWQvcvn1b6paTk8P9r9AVFBQgOTkZycnJkJeXx8iRI/Hnn3/C0tISK1eu5DsegBdHqWKxGD169EBRURF3XywWo6KiApmZmfjwww/5jiklOzu7zlXatLS0kJub2+55GktdXR2PHj0CABw9ehQffPABAEBFRQXPnz/nM1q9DAwMcOPGDdTU1ODIkSNc5rKyMsjLy/Ocrm5r1qzBDz/8gP/85z9QUlLC/PnzkZycjNmzZ+PZs2d8x5Ny9uxZODo6SrU7Ojri/PnzPCRqGQMDA2RmZvIdo/H4/qVA2ldlZSXbt28fGzVqFFNUVGQODg5s7dq17NmzZ9w2SUlJTFtbm8eUkiorK9l7770nqCP91xk8eDD74IMPWGFhIddWWFjIhg8fzoYMGcJjsoZNnDiR2dvbM19fX6ampsYePnzIGGPswIED7O233+Y5Xd3CwsKYlpYW69u3L+vevTsrLy9njDG2ceNG9u677/Kcrm6qqqosNzeXMcaYnp4eu3r1KmOMsVu3bjEdHR0+o9WpT58+bN68eVLt8+bNY3369OEhUeOkp6dL3K5evcoOHz7MXFxc2MCBA/mO12h0jrqFtm3bhoSEBNy+fRtnz55Fjx49EBcXBzMzM4wZM4bveFK6du0KsVgMLy8vnD9/Hra2tlLbDBs2rM3X7G4KRUVFXLt2je8YTbJx40Z4enqie/fuMDExAQDk5+dzq70JVXx8PIKDg5Gfn4+ffvqJG2V/6dIleHl58ZyubkuWLME777yD/Px8jB8/nlt0QV5eHgsXLuQ5Xd0MDQ3x+PFj9OjRA927d8e5c+dgY2OD27dvSwxAFIqVK1fi448/xuHDh+Hk5AQAOH/+PP7++2/89NNPPKern62trdSgTgB49913kZiYyFOqpqPLs1pg7dq1CA0Nxdy5cxEVFYXr16+jZ8+e2Lx5M7Zs2SIxGEMotm3bhvHjx0NFRYXvKE0SEBAAZWVlLF26lO8ojcYYQ3JyMm7evAkAsLCwgKura6NH0pKmKy8vl4nv9vTp02FiYoKwsDDEx8dj3rx5GDhwIC5evAhPT09s3LiR74hS/vnnH6xduxYZGRkAXnyfZ8yYwf0QFaI7d+5I3JeTk4Oenp5MfEdeRoW6BSwtLREdHY2xY8dCQ0MD6enp6NmzJ65fv46hQ4fi4cOHfEeUUFVVBVVVVVy9elXm1u+eNWsWtm7dit69e9c5wj42NpanZNJk+XMGgJMnT2LdunXIycnBjz/+CGNjY2zbtg1mZmYYNGgQ3/Gk1NTUIDo6GgkJCbh//z5u3bqFnj17IiQkBKampvD19eU7opTacRYKCi86NXfv3o0zZ86gd+/e+Pzzz6GkpMRzwv+pqqqCu7s7EhIS0Lt3b77jvJFoMFkL3L59G3Z2dlLtysrKKC0t5SFRwxQVFdG9e3eZuXbwZdevX4e9vT00NDRw69YtXLlyhbtdvXqV73gSZPlz/umnn+Dm5gZVVVVcvnwZFRUVAF5cfy/Uy8qioqKwefNmLFu2TKLAvfPOO9iwYQOPyeonJyfHFWkA+PTTT7F69WrMmjVLUEUakM1TTy87ceIEPDw8YG5uDnNzc4wePRonT57kO1bT8Hh+XOZZWFiw/fv3M8YYU1dXZ9nZ2YwxxlavXs3s7Oz4jFavDRs2sJEjR7JHjx7xHaVDk9XP2dbWlm3ZsoUxJvmdvnz5MjMwMOAzWr169erFjh07xhiTzJyRkSGoQZEvMzMzY1OnTuUGvtUqKipiZmZmPKWq39y5c9mCBQv4jtFk27ZtYwoKCuyTTz5hq1atYqtWrWKffPIJU1RUZDt27OA7XqPRYLIWCAwMxMyZM1FeXg7GGM6fP49du3YhJiZGsL/kv//+e2RlZcHIyAg9evSQ6kIWwkQLr/PPP/8AALp168ZzkvrJ6uecmZlZ57SKWlpaePr0afsHaoS7d+/C3Nxcql0sFqOqqoqHRK+Xm5sLBQUFDB48GL/88gsMDQ0BvOjGf/W8qhBUV1cjMTERx44dE/ypp5dFRUVh2bJlCAgI4Npmz56N2NhYREZGYuLEiTymazwq1C0wffp0qKqqIjg4GGVlZZg4cSKMjIywatUqfPrpp3zHq9Or01zKCrFYjG+++QYrVqxASUkJAEBDQwNfffUVvv76a8jJCessjqx+zoaGhsjKypKa7e3UqVPo2bMnP6Few9LSEidPnpSa3nTfvn11npoSApFIhCNHjiAoKAgODg7Yv38/+vXrx3esetWeegKAW7duSTwm5MGROTk58PDwkGofPXo0Fi9ezEOiZuL7kL6jKC0tZffv3+c7Roe1cOFCpqenx9asWcNdExkfH8/09PQEOZOTrIqOjmaWlpbs3LlzTENDg508eZJt376d6enpsdWrV/Mdr0779+9nWlpabOnSpUxNTY0tX76cTZ8+nSkpKbGjR4/yHa9OIpGI+/di4cKFTFVVlW3bto0VFhbKzKyGsqBXr14sISFBqn3t2rXM3Nych0TNQ4W6BcrKylhpaSl3Pzc3l61cuZL9/vvvPKZ6vSdPnrD169ezhQsXcudQL126xP755x+ek9Wva9eu7MCBA1Lt+/fvZ0ZGRjwk6pjEYjH75ptvWKdOnbipWlVUVFhwcDDf0RqUlpbGXF1dmZ6eHlNVVWUDBw4U9H+HcnJyEj/st23bxlRUVJiPjw8V6la0Zs0apqSkxGbMmMG2bt3Ktm7dyj7//HOmrKxcZwEXKro8qwWGDx8OT09PzJgxA0+fPsVbb70FJSUlPHz4ELGxsfjiiy/4jijl2rVrcHV15aayzMzMRM+ePREcHIy8vDxs3bqV74h1UlFRwbVr19CnTx+J9szMTNja2gpuesuamhqsXLmy3kUXHj9+zFOyxqmsrERWVhZKSkpgaWkJdXV1viN1KHJycigsLIS+vj7XdvbsWXz00UcoKioS5BUDFy9erPf7LMTFWmr9/PPPWLFihcT13/PmzRPkhFT14vuXgizr0qULu379OmOMsfXr1zNra2tWU1PD9u7dK9iFF95//31uKsCXR8iePn2a9ejRg8dkDevfvz+bNWuWVLu/vz9zcnLiIVHDQkJCWNeuXdl3333HVFRUWGRkJPP19WVdunRhq1at4jteh+Lr68uOHz/Od4xWUVhYyFJTU/mOIWXXrl1MUVGRffjhh0xJSYl9+OGHrE+fPkxLS4tNnTqV73j18vb2ZidOnOA7RotRoW6Bl1caGj9+PFuyZAljjLG8vDymqqrKZ7R6aWpqsqysLMaYZKHOzc1lysrKfEZrUGpqKuvUqROzsLBg06ZNY9OmTWMWFhZMXV2dpaWl8R1PSs+ePdnBgwcZYy8+59rPfNWqVczLy4vPaA0qKSlhwcHBzNnZmfXq1YuZmZlJ3IRo9OjRTFlZmXXr1o0FBQWxK1eu8B3ptcLDw1lKSopUe0lJCQsPD+chUcOsrKzY999/zxj7378bYrGY+fn5sdDQUJ7T1W/MmDFMUVGRmZubs6ioKHb37l2+IzULFeoWsLKyYqtWrWJ5eXlMU1OTnTlzhjHG2MWLFwV7zamenh67fPkyY0yyUB89epR169aNz2ivdffuXbZ48WLm6enJPD092ddffy3Y//DU1NS4H3GGhobs0qVLjDHGsrOzmaamJp/RGvTpp5+yrl27svnz57OVK1eyuLg4iZtQPX78mK1bt465uLgwOTk5ZmlpyaKiotjt27f5jlan2mVaV6xYIdEu1MFkampq3Gepo6PDrl27xhhj7MaNG8zQ0JDHZK/34MEDtmLFCmZtbc0UFBSYu7s727t3L6usrOQ7WqNRoW6BH3/8kSkqKjI5OTnm6urKtUdHRzN3d3cek9XP19eXjR07llVWVjJ1dXWWk5PD7ty5w+zs7Lh1fIXio48+4lb12rJli9TkEELWp08fdu7cOcYYYwMHDmQxMTGMMcZ2797N9PT0+IzWIC0tLXbq1Cm+Y7RIfn4+W7ZsGevbty+Tl5fnO06dRCIR2717N+vSpQubOnUqq6ioYIwJt1AbGxtzxdnKyopbm/rMmTOC/uH5qkuXLjF/f3+moqLCdHV12dy5c2ViVT4q1C1UUFDALl++zGpqari2P/74g2VkZPCYqn5Pnz5lrq6uTFtbm8nLyzMTExOmqKjIhgwZwkpKSviOJ0FRUZHdu3ePMSY9SlboFixYwKKiohhjL4qzgoICMzc3Z0pKSoKe4cnU1JTduHGD7xjNVllZyX7++Wf28ccfMxUVFcFeEVB7eVZWVhazsLBgzs7O7P79+4It1F5eXtzRf0REBNPT02PTp09nPXr0YB999BHP6Rrn3r17bOnSpeytt95inTp1Yt7e3uz9999nCgoKLDY2lu94DaJR361EFmbLetmpU6dw7do1lJSUwN7eHq6urnxHkmJtbQ17e3sMGzYMPj4+WL16NTQ1Nevc1tvbu53TNc25c+e4RRfqmoBBKLZv344DBw5gy5YtUFNT4ztOox0/fhw7d+7ETz/9BLFYDE9PT0yaNAnvvfeeICfkkJeXR0FBAfT19VFcXIxPPvkEf/31FxISEjB69GjBjfp+/PgxysvLYWRkBLFYjGXLlnHf5+DgYHTu3JnviHWqqqrCL7/8gk2bNuHo0aOwtrbG9OnTMXHiRO7fkp9//hnTpk3DkydPeE5bPyrULSBrs2UBL9ZEFvKydC87ffo0vvrqK2RnZ+Px48fQ0NCo8x9dkUgk+MudhMzOzk7ic83KygJjDKamplBUVJTYVohTnxobG+Px48dwd3fHpEmT4OHhwa1JLVSvXp4lFosxd+5crF27FmKxWHCFWlbp6upCLBbDy8sLfn5+sLW1ldrm6dOnsLOzw+3bt9s/YCPRFKIt8PXXX2Pjxo1YunQpBg4cCODFkeqSJUtQXl6OqKgonhNKMzU1xaBBgzB58mSMGzdOsL+EAWDgwIE4d+4cgBf/sN26dUviulMh6969O4YOHQoXFxcMHToUvXr14jtSvWR1utNaS5Yswfjx46Gtrc13lEbbtGkTtLS0uPtycnJYvXo17OzskJaWxmOyunl7e2PYsGEYMmSIoL/Lr1q5ciXGjx/f4PrT2tragi7SAB1Rt4iRkRHXVfWyAwcO4Msvv8Tdu3d5Sla/K1euYOfOndi9ezeKiorg7u6OyZMnC/IoxNPTE5s3b4ampia2bNmCTz75BKqqqnzHapTt27cjLS0NqampyMrKgrGxMVxcXLjCTev6tg1ZOwUlK6ZPn460tDSJ73LtD1H6Lrc9KtQtIGuzZb2MMYbU1FSp83qJiYl8R+MoKSnhzp076Nq1q8Q5PVlTUFCAEydO4ODBg9izZ4+guzYvXLgAsVgMJycnifY//vgD8vLycHR05ClZ/WTlFNTq1avxf//3f1BRUcHq1avr3U4kEmHWrFntmKzx7t69i7S0NJw4cQInTpzArVu30LVrV+4HEmkbVKhbwMnJCU5OTlL/0c2aNQsXLlzgum2F7vLly/D19cW1a9cEVUBkfTBZWVkZTp06hdTUVBw/fhxXrlyBhYUFhg4dipUrV/Idr079+/fH/PnzMW7cOIn2pKQkfPvtt/jjjz94Sla/RYsWYePGjQgPD5c6BeXn5yeYU1BmZma4ePEiunTpAjMzs3q3E4lEyMnJacdkjVf7nT5+/DhSU1Nx+fJlWFpa4sqVK3xH69CoULfAiRMnMGrUKHTv3h3Ozs4AXszXm5+fj99++w2DBw/mOWH9/vnnH+zcuRM7d+7E9evX4ezsjEmTJmHGjBl8R+OcOXMGgYGBMjmYbMCAARKF2cXFBUOGDBH0mAAAUFdXx7Vr16SWtLx9+zasra3x77//8pSsfrJ4Cupltf8EC3F0eq3FixcjNTWV+07Xdn3Lwne6I6BC3UL37t1DfHw8bt68CeDFhO9ffvkljIyMeE5Wt3Xr1mHnzp04deoULCwsMGnSJEycOFFqLV+hqWsRAyHT0dGBnJwchg8fjqFDh2Lo0KFSp0iEqEuXLjh48CD3w7PWmTNnMGrUKEFewiKrp6A2btyIlStX4u+//wYA9O7dG3PnzsX06dN5TiZNTk4Oenp6CAgIgKenp0x8lzsSKtRvGBMTE3h5eWHSpEmwsbHhO06j3blzB3l5eVi3bh1ycnLw448/wtjYGNu2bYOZmRkGDRrEd0QJjDH8+eefSE1NxYkTJ5CWlgYlJSW4uLhg2LBh8PPz4ztinby8vFBQUIADBw5wo5KfPn2KsWPHQl9fH3v37uU5oTRZPAUVGhqK2NhYzJo1S6I37vvvv0dAQAAiIiJ4TigpPT0dJ06cQGpqKk6ePMl9l2XpR6gso0LdRNeuXWv0ttbW1m2YpHkYYzh16pTMFLxaP/30Ez777DNMmjQJ27Ztw40bN9CzZ098//33+O233/Dbb7/xHbFejDFcunQJ33//PXbs2CHowWR3797FkCFD8OjRI9jZ2QEArl69CgMDAyQnJwvyGvz6TkHl5eXh8OHDgjwFpaenh9WrV8PLy0uifdeuXZg1axYePnzIU7LGSU9Px8qVKwX/fe4o6DrqJrK1tYVIJMLrft+IRCJBfnmTkpK4gnf58mVUVFQAAJ49e4bo6GjBFrxvvvkGCQkJ8Pb2xu7du7n2gQMH4ptvvuExWd0uX76M1NRUpKam4tSpU/j3339hZWWFWbNmwcXFhe949TI2Nsa1a9ewY8cOpKenQ1VVFT4+PvDy8pKa/EQoXFxckJmZibVr13JrDnt6egr6FFRVVVWdI+gdHBxQXV3NQ6KGMcZw5coVie90cXExrK2tBf197ijoiLqJ7ty50+hthXje187ODgEBAfD29oaGhgbS09PRs2dPXLlyBSNGjEBhYSHfEeukpqaGGzduwNTUVCJ3Tk4OLC0tUV5ezndECQoKCrCzs+OunR4yZIjEBBekdZWXl+PatWt48OABxGKxxGOvDjITglmzZkFRURGxsbES7UFBQXj+/Dni4+N5Sla3zp07o6SkBDY2NlyX9+DBg2VqkhlZRkfUTfRy8Y2JiYGBgQGmTZsmsU1iYiKKioqwYMGC9o73WpmZmRgyZIhUu5aWFp4+fdr+gRrJ0NAQWVlZMDU1lWg/deqU1AhlvtXU1CApKQmDBw+WyRGxf//9N44fP15n0QsNDeUpVf2OHDkCb29vPHr0SKqnS6g9W8CLwWRHjx7Fu+++C+DFtep5eXnw9vZGYGAgt92rxZwP27dvx+DBg+u9PJK0LSrULVA7gvpVb7/9Nj799FNBFmpZKngv8/Pzw5w5c5CYmAiRSIR79+7h7NmzCAoKQkhICN/xJMjLy+OTTz5BRkaGzBXq9evX44svvoCuri4MDQ0lLhkSiUSCLNSzZs3C+PHjERoaCgMDA77jNMr169dhb28PAMjOzgbwYl5qXV1dXL9+ndtOKJdsjRo1ivubZn/jQbus0dVBKSsrs5ycHKn27OxspqyszEOi14uOjmaWlpbs3LlzTENDg508eZJt376d6enpsdWrV/Mdr15isZh98803rFOnTkwkEjGRSMRUVFRYcHAw39Hq5ODgwI4dO8Z3jCbr3r07W7p0Kd8xmkRDQ4NlZWXxHaNDq6mpYeHh4UxTU5PJyckxOTk5pqWlxSIiIiSW+CVtgwp1C5ibm7Nt27ZJtW/dupWZmZnxkOj1ZK3gvaqiooL99ddf7I8//mD//vsv33HqdfjwYWZra8t+/fVXdu/ePfbs2TOJm1BpaGiw7OxsvmM0iY+PD9uwYQPfMTq0hQsXMj09PbZmzRqWnp7O0tPTWXx8PNPT02OLFy/mO16HR4PJWmDZsmVYtmwZli9fjvfeew8AkJKSgvnz5+Orr77CokWLeE5Yv8rKSmRlZaGkpASWlpZQV1fnO1KH8vL80i93XzLGBH3e1NfXF/369RPUDHWvU1ZWhvHjx0NPTw9WVlZSo9Nnz57NU7KOQ9Znf5N1dI66BebNm4dHjx7hyy+/RGVlJYAXsyQtWLBA0EUaeLHghaWlJd8xOqzjx4/zHaFZzM3NERISgnPnzslM0du1axeOHj0KFRUVpKamSp1XF2JmWfP48WP07dtXqr1v376Cm763I6Ij6lZQUlKCjIwMqKqqonfv3oJbLpKQxpLFxSIMDQ0xe/ZsLFy4UDArZXU0sjj7W0dChZqQNvL06VNs3LiRm4Tj7bffxrRp0+h66lamo6ODCxcuoFevXnxH6bBkeQGijoAKNSFt4OLFi3Bzc4Oqqir69+8P4MVaz8+fP8fRo0e5S3OEIDAwEJGRkejUqZPE9buvEolEWLFiRTsma5yAgADo6elh8eLFfEfpsPLy8qCgoFDnAkTV1dXo3r07zwk7NirUhLSBwYMHw9zcHOvXr4eCwouhINXV1Zg+fTpycnKQlpbGc8L/GTZsGH7++Wdoa2tj2LBh9W4nEonw3//+tx2TNc7s2bOxdetW2NjYwNraWuq8uhAmDJF18vLyKCgokFq97tGjR9DX1xfs4MiOggo1IW1AVVUVV65ckRqAc+PGDTg6OqKsrIynZB2PLP64kDX1LTN7584dWFpaorS0lKdkbwYa9U1IG9DU1EReXp5Uoc7Pz4eGhgZPqTomWR1hLwtqT4XUzkqnpqbGPVZTU4M//vgDtra2PKV7c1ChJqQNTJgwAb6+vvjuu+8wYMAAAMDp06cxb948qaUNCRGqK1euAPjf+upKSkrcY0pKSrCxsUFQUBBf8d4Y1PVNSCu5du0a3nnnHcjJyaGyshLz5s1DQkICt2yhoqIivvjiCyxdupQu4SMyxcfHB6tWraJFOXhChZqQVvLygJuePXviwoULUFVV5RZd6NWrl0TXISGENAZ1fRPSSrS1tXH79m3o6+sjNzcXYrEYampqsLKy4jsaIUSGUaEmpJV8/PHHcHFxQdeuXSESieDo6Ah5efk6txXiDF+EEGGiQk1IK/nhhx/g6emJrKwszJ49G35+fjTCmxDSYnSOmpA24OPjg9WrV1OhJoS0GBVqQgghRMBoqRlCCCFEwKhQE0IIIQJGhZoQQggRMCrUhBBCiIBRoSaEEEIEjAo1IYQQImBUqAkhhBABo0JNCCGECNj/AziNpZr5Sbj4AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d750e989-842a-4cfa-a44b-cf44d6e49163",
   "metadata": {},
   "source": [
    "- We can see that the rescaling via temperature 0.1 results in a sharper distribution, approaching `torch.argmax`, such that the most likely word is almost always selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4600713-c51e-4f53-bf58-040a6eb362b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "992 x forward\n",
      "0 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "8 x toward\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526e93cb-8e2a-42a1-b1ba-4fd5fe64c26b",
   "metadata": {},
   "source": [
    "- The rescaled probabilities via temperature 5 are more uniformly distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9dfb48f0-bc3f-46a5-9844-33b6c9b0f4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153 x closer\n",
      "68 x every\n",
      "55 x effort\n",
      "223 x forward\n",
      "102 x inches\n",
      "50 x moves\n",
      "43 x pizza\n",
      "218 x toward\n",
      "88 x you\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c83f0c4-3774-4375-ad7f-96440ba5fef7",
   "metadata": {},
   "source": [
    "- Assuming an LLM input \"every effort moves you\", using the approach above can sometimes result in nonsensical texts, such as \"every effort moves you pizza\", 3.2% of the time (32 out of 1000 times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e4873e-07e4-4abb-85df-bdaedcc1a6f7",
   "metadata": {},
   "source": [
    "### 5.3.2 Top-k sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4da95a-8bb2-4f69-a9b0-a643531db5df",
   "metadata": {},
   "source": [
    "- To be able to use higher temperatures to increase output diversity and to reduce the probability of nonsensical sentences, we can restrict the sampled tokens to the top-k most likely tokens:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae6fffd-2730-4abe-a2d3-781fc4836f17",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/topk.webp\" width=500px>\n",
    "\n",
    "- (Please note that the numbers in this figure are truncated to two\n",
    "digits after the decimal point to reduce visual clutter. The values in the Softmax row should add up to 1.0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba12da5-6ff1-4008-91b8-d2d537cbc14c",
   "metadata": {},
   "source": [
    "- In code, we can implement this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2a7f908a-e9ec-446a-b407-fb6dbf05c806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "753865ed-79c5-48b1-b9f2-ccb132ff1d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float('-inf')), \n",
    "    other=next_token_logits\n",
    ")\n",
    "\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4844f000-c329-4e7e-aa89-16a2c4ebee43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56056503-a15d-4315-a3ff-46647a4c7c45",
   "metadata": {},
   "source": [
    "### 5.3.3 Modifying the text generation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34770423-473d-46f6-a5fa-6b2979564d26",
   "metadata": {},
   "source": [
    "- The previous two subsections introduced temperature sampling and top-k sampling\n",
    "- Let's use these two concepts to modify the `generate_simple` function we used to generate text via the LLM earlier, creating a new `generate` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8e318891-bcc0-4d71-b147-33ce55febfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aa2a0d7d-0457-42d1-ab9d-bd67683e7ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know began to my surprise, a little it was the\n",
      "\"Ah enough\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2002ca-f4c1-48af-9e0a-88bfc163ba0b",
   "metadata": {},
   "source": [
    "## 5.4 Loading and saving model weights in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc52676-f026-4566-a226-2a90269f9d53",
   "metadata": {},
   "source": [
    "- Training LLMs is computationally expensive, so it's crucial to be able to save and load LLM weights\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model-3.webp\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e4c7f9-592f-43d6-a00e-598fa01dfb82",
   "metadata": {},
   "source": [
    "- The recommended way in PyTorch is to save the model weights, the so-called `state_dict` via by applying the `torch.save` function to the `.state_dict()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3d67d869-ac04-4382-bcfb-c96d1ca80d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e889e0-07bf-43e5-8f92-5c5c7aeaad9e",
   "metadata": {},
   "source": [
    "- Then we can load the model weights into a new `GPTModel` model instance as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9d57d914-60a3-47f1-b499-5352f4c457cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa81aec-9c72-4f46-8ae2-4a4fde3edbc1",
   "metadata": {},
   "source": [
    "- It's common to train LLMs with adaptive optimizers like Adam or AdamW instead of regular SGD\n",
    "- These adaptive optimizers store additional parameters for each model weight, so it makes sense to save them as well in case we plan to continue the pretraining later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bbd175bb-edf4-450e-a6de-d3e8913c6532",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8a0c7295-c822-43bf-9286-c45abc542868",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4194350e-0409-4a63-8ffd-d3a896509032",
   "metadata": {},
   "source": [
    "## 5.5 Loading pretrained weights from OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eb6c38-7278-40e0-bd9f-8a2b1feac3ec",
   "metadata": {},
   "source": [
    "- Previously, we only trained a small GPT-2 model using a very small short-story book for educational purposes\n",
    "- Interested readers can also find a longer pretraining run on the complete Project Gutenberg book corpus in [../03_bonus_pretraining_on_gutenberg](../03_bonus_pretraining_on_gutenberg)\n",
    "- Fortunately, we don't have to spend tens to hundreds of thousands of dollars to pretrain the model on a large pretraining corpus but can load the pretrained weights provided by OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127ddbdb-3878-4669-9a39-d231fbdfb834",
   "metadata": {},
   "source": [
    "- For an alternative way to load the weights from the Hugging Face Hub, see [../02_alternative_weight_loading](../02_alternative_weight_loading)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cab892-a165-4f43-9601-f517bc212ab6",
   "metadata": {},
   "source": [
    "- First, some boilerplate code to download the files from OpenAI and load the weights into Python\n",
    "- Since OpenAI used [TensorFlow](https://www.tensorflow.org/), we will have to install and use TensorFlow for loading the weights; [tqdm](https://github.com/tqdm/tqdm) is a progress bar library\n",
    "- Uncomment and run the next cell to install the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb9fdf02-972a-444e-bf65-8ffcaaf30ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0747edc-559c-44ef-a93f-079d60227e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.15.0\n",
      "tqdm version: 4.66.2\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", version(\"tensorflow\"))\n",
    "print(\"tqdm version:\", version(\"tqdm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c5bc89eb-4d39-4287-9b0c-e459ebe7f5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative import from the gpt_download.py contained in this folder\n",
    "from gpt_download import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff76a736-6f9f-4328-872e-f89a7b70a2cc",
   "metadata": {},
   "source": [
    "- We can then download the model weights for the 124 million parameter model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "76271dd7-108d-4f5b-9c01-6ae0aac4b395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77.0/77.0 [00:00<00:00, 58.8kiB/s]\n",
      "encoder.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.04M/1.04M [00:00<00:00, 2.70MiB/s]\n",
      "hparams.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90.0/90.0 [00:00<00:00, 27.8kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 498M/498M [00:30<00:00, 16.1MiB/s]\n",
      "model.ckpt.index: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.21k/5.21k [00:00<00:00, 1.18MiB/s]\n",
      "model.ckpt.meta: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 471k/471k [00:00<00:00, 2.22MiB/s]\n",
      "vocab.bpe: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 2.04MiB/s]\n"
     ]
    }
   ],
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b1a31951-d971-4a6e-9c43-11ee1168ec6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "857c8331-130e-46ba-921d-fa35d7a73cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c48dac94-8562-4a66-84ef-46c613cdc4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466e100c-294e-4afc-a70a-2f398ac4c104",
   "metadata": {},
   "source": [
    "- Alternatively, \"355M\", \"774M\", and \"1558M\" are also supported `model_size` arguments\n",
    "- The difference between these differently sized models is summarized in the figure below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f19d32-5aae-4176-9f86-f391672c8f0d",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/gpt-sizes.webp?timestamp=123\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6e5076-f08d-41fc-bd8b-1cfe53538f41",
   "metadata": {},
   "source": [
    "- Above, we loaded the 124M GPT-2 model weights into Python, however we still need to transfer them into our `GPTModel` instance\n",
    "- First, we initialize a new GPTModel instance\n",
    "- Note that the original GPT model initialized the linear layers for the query, key, and value matrices in the multi-head attention module with bias vectors, which is not required or recommended; however, to be able to load the weights correctly, we have to enable these too by setting `qkv_bias` to `True` in our implementation, too\n",
    "- We are also using the `1024` token context length that was used by the original GPT-2 model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9fef90dd-0654-4667-844f-08e28339ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configurations in a dictionary for compactness\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f29ac-8342-4b3d-a57d-9b0166ced314",
   "metadata": {},
   "source": [
    "- The next task is to assign the OpenAI weights to the corresponding weight tensors in our `GPTModel` instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f9a92229-c002-49a6-8cfb-248297ad8296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f22d5d95-ca5a-425c-a9ec-fc432a12d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "    \n",
    "    \n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7472cb-54dc-4311-96d8-b2694f885cee",
   "metadata": {},
   "source": [
    "- If the model is loaded correctly, we can use it to generate new text using our previous `generate` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1f690253-f845-4347-b7b6-43fabbd2affa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you toward finding an ideal new way to practice something!\n",
      "\n",
      "What makes us want to be on top of that?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d079f98-a7c4-462e-8416-5a64f670861c",
   "metadata": {},
   "source": [
    "- We know that we loaded the model weights correctly because the model can generate coherent text; if we made even a small mistake, the mode would not be able to do that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28493b9b-a1ae-4f31-87bc-c10ee4447f44",
   "metadata": {},
   "source": [
    "- For an alternative way to load the weights from the Hugging Face Hub, see [../02_alternative_weight_loading](../02_alternative_weight_loading)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a66474-230d-4180-a8ff-843e04f1f1c4",
   "metadata": {},
   "source": [
    "## Summary and takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7ed189-a633-458c-bf12-4f70b42684b8",
   "metadata": {},
   "source": [
    "- See the [./gpt_train.py](./gpt_train.py) script, a self-contained script for training\n",
    "- The [./gpt_generate.py](./gpt_generate.py) script loads pretrained weights from OpenAI and generates text based on a prompt\n",
    "- You can find the exercise solutions in [./exercise-solutions.ipynb](./exercise-solutions.ipynb)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
